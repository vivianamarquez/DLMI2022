{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **UNET Segmentation TRAINING**  \n",
    "### *Keras / Tensorflow<span style=\"color:brown\">  2*\n",
    "\n",
    "*thomas.grenier@creatis.insa-lyon.fr*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train a UNet network.\n",
    "We first need to go deeper in this architecture.\n",
    "\n",
    "### <span style=\"color:brown\"> **UNet**\n",
    "This network is fully convolutional. It uses skip connections from the encoder side to the decoder side to preserve scale information.\n",
    "\n",
    "This basic architecture can be changed in order to best perform on your data. \n",
    "In the following figure, the UNet is a 4 stages (or levels) UNet with 32 neurons on the first layer. These two features are the most significant for this architecture.\n",
    "\n",
    "The encoder side consists of 2 convolution layers (convolution + activation + batch normalization) at each level.\n",
    "Each level has twice more neurons than the previous one.\n",
    "To jump from on level to the next one, a max pooling is performed which reduces the spatial size by 4.\n",
    "\n",
    "The decoder processes symmetrically.\n",
    "At each level, the features coming from the encoder level aside is concatenated to the features coming from the lower decoder level.\n",
    "Then, as for the encoder, decoder levels consist of 2 convolution layers.\n",
    "To adapt the spatial size from one stage to thee next one, an upsampling is performed.\n",
    "This upsampling can be done by interpolation (https://keras.io/api/layers/reshaping_layers/up_sampling2d/) or by transposed convolution (https://keras.io/api/layers/convolution_layers/convolution2d_transpose/) that someway learnt how to interpolate.\n",
    "\n",
    "To produce the segmentation, all feature are first sumup to a *n* features map, where *n* is the number of classes, by *n* 1x1 convolution layer.\n",
    "Then a *arg max* is done to produce the segmentation in the multi-classe case.\n",
    "For binary segmentation, a simple threshold does the job.\n",
    "\n",
    "<img src=\"figures/UNet.png\" alt=\"UNet\" style=\"width: 70%;\"/>\n",
    "\n",
    "In this UNet implementation, many parameters can be modified:\n",
    "- The number of levels \n",
    "- The amount of feature at first level.\n",
    "- Batch normalization can be activated or not\n",
    "- Dropout amount (0.0 means no dropout)\n",
    "- The upsampling strategy\n",
    "- Activation functions\n",
    "- Number of classes\n",
    "\n",
    "Then, many hyper-parameters can be tuned for training.\n",
    "\n",
    "In this notebook, you will train a UNet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Questions\n",
    "\n",
    "- what was the architecture of the previously used network (in the test notebook) ? number of stages, features at first stage?...\n",
    "- how many parameters have this network ?\n",
    "- compare the code function *conv2d_block* of the network (located in keras_unet/models/custom_unet.py) with the network description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **1- System and basic imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:08.991851Z",
     "start_time": "2019-06-11T09:30:08.985860Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "import cv2\n",
    "    \n",
    "#prevent unwanted warning \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **2- Important parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE =  96  # 96 x 96 input images\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "\n",
    "JUPYTER_DISPLAY_ON = True # True or False used for displaying (or not) images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture related values \n",
    "NBLAYERS_UNET = 5      # <- number of levels [5]\n",
    "NBFILTERS_L1_UNET = 32 # <- number of neurons for the first level [32]\n",
    "DROPOUT_RATE = 0.1     # 0.0  to 1.0 [0.1]\n",
    "KERNEL_SIZE = (3,3)    # (3,3)  (5,5) \n",
    "BATCHNORM_ON = False    # True or False [False]\n",
    "\n",
    "CNN_ACTIVATION = 'relu' # relu, elu, selu, LeakyReLU, ...\n",
    "\n",
    "# Training parameters\n",
    "NBEPOCHS = 20          # Nb of Epoch  [20]\n",
    "BATCH_SIZE = 16         # Number of sample in each batch (4 to 64) [16]\n",
    "NBSTEPS_PER_EPOCH = 50  # nb of batches per epoch (1  to ...)  [50] (used for data augmentation)\n",
    "NBPATIENCE_EPOCHS = 30  # nb of epoch after a minimum detection before stopping (early stop) [30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './trained_model/'\n",
    "model_name = 'Unet_1i_f' + str(NBFILTERS_L1_UNET) + '_l'+ str(NBLAYERS_UNET) +  '_k'+ str(KERNEL_SIZE[0]) + '_do' + str(DROPOUT_RATE) +'_act'+CNN_ACTIVATION + '_b' + str(BATCH_SIZE) +'_Std'\n",
    "if BATCHNORM_ON == True:\n",
    "    model_name = model_name + '_BN'\n",
    "model_filename = model_path + model_name + '_input'+ str(IMG_SIZE) +'.h5'\n",
    "print(\" -> model_study : \", model_name)\n",
    "print(\" -> model_filename : \", model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **3- Session launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Number of GPUs available : \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **4- Load data**\n",
    "\n",
    "<span style=\"color:red\">\n",
    "    \n",
    "**Before running the next cell**, you must have :\n",
    "   1. downloaded the file ```dlss21_ho4_data.tar.gz``` \n",
    "   2. unziped the downloaded file in the directory of this notebook.\n",
    "    \n",
    "Data should be at ```./dlss21_ho4_data```    \n",
    "    \n",
    "## <span style=\"color:brown\"> 4.1- Load files and set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:09.006881Z",
     "start_time": "2019-06-11T09:30:08.994853Z"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_masks_files     = glob.glob(\"./dlss21_ho4_data/train/labels/*.png\")\n",
    "train_images_e8_files = glob.glob(\"./dlss21_ho4_data/train/images/*_e8.png\")\n",
    "train_images_e5_files = glob.glob(\"./dlss21_ho4_data/train/images/*_e5.png\")\n",
    "train_images_e2_files = glob.glob(\"./dlss21_ho4_data/train/images/*_e2.png\")\n",
    "\n",
    "# test\n",
    "test_masks_files     = glob.glob(\"./dlss21_ho4_data/test/labels/*.png\")\n",
    "test_images_e8_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e8.png\")\n",
    "test_images_e5_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e5.png\")\n",
    "test_images_e2_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e2.png\")\n",
    "\n",
    "\n",
    "os.makedirs(model_path+model_name, exist_ok=True)  # save fig\n",
    "\n",
    "logs_path = '../logs/'\n",
    "os.makedirs(logs_path, exist_ok=True)  # save fig\n",
    "\n",
    "train_images_e2_files.sort()\n",
    "train_images_e5_files.sort()\n",
    "train_images_e8_files.sort()\n",
    "train_masks_files.sort()\n",
    "\n",
    "test_images_e2_files.sort()\n",
    "test_images_e5_files.sort()\n",
    "test_images_e8_files.sort()\n",
    "test_masks_files.sort()\n",
    "\n",
    "print( \" trainning :  \", len(train_images_e2_files), len(train_images_e5_files), len(train_images_e8_files), len(train_masks_files) )\n",
    "print( \" testing   :  \", len(test_images_e2_files), len(test_images_e5_files), len(test_images_e8_files), len(test_masks_files) )\n",
    "\n",
    "nb_train = int( len(train_masks_files) * (1 - VALIDATION_RATIO) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The previous cell should output 717 as training images and 208 as testing images.\n",
    "If not, you must check where the data is.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.2- Randomly permute the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:09.006881Z",
     "start_time": "2019-06-11T09:30:08.994853Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_display = 10\n",
    "if JUPYTER_DISPLAY_ON == True:\n",
    "    print( f\" -> First {nb_display} train items \")\n",
    "    [print(train_images_e2_files[i], train_images_e5_files[i], train_images_e8_files[i], train_masks_files[i]) for i in range(nb_display)]\n",
    "\n",
    "    print( f\" -> Last {nb_display} train items \")\n",
    "    [print(train_images_e2_files[i], train_images_e5_files[i], train_images_e8_files[i], train_masks_files[i]) for i in range( len(train_masks_files)-nb_display, len(train_masks_files) )]\n",
    "\n",
    "# permuation of inputs\n",
    "permutation_train = np.random.permutation( len(train_masks_files))\n",
    "\n",
    "# print(\"Permutation : \", permutation)\n",
    "train_images_e2_files_rnd=[train_images_e2_files[i] for i in permutation_train]\n",
    "train_images_e5_files_rnd=[train_images_e5_files[i] for i in permutation_train]\n",
    "train_images_e8_files_rnd=[train_images_e8_files[i] for i in permutation_train]\n",
    "train_masks_files_rnd=[train_masks_files[i] for i in permutation_train]\n",
    "\n",
    "if JUPYTER_DISPLAY_ON == True:\n",
    "    print( f\" -> First PERMUTED {nb_display} train items \")\n",
    "    [print(train_images_e2_files_rnd[i], train_images_e5_files_rnd[i], train_images_e8_files_rnd[i], train_masks_files_rnd[i]) for i in range(nb_display)]\n",
    "\n",
    "    print( f\" -> Last PERMUTED {nb_display} train items \")\n",
    "    [print(train_images_e2_files_rnd[i], train_images_e5_files_rnd[i], train_images_e8_files_rnd[i], train_masks_files_rnd[i]) for i in range( len(train_masks_files)-nb_display, len(train_masks_files) )]\n",
    "\n",
    "permutation_test = np.random.permutation( len(test_masks_files))\n",
    "test_images_e2_files_rnd=[test_images_e2_files[i] for i in permutation_test]\n",
    "test_images_e5_files_rnd=[test_images_e5_files[i] for i in permutation_test]\n",
    "test_images_e8_files_rnd=[test_images_e8_files[i] for i in permutation_test]\n",
    "test_masks_files_rnd=[test_masks_files[i] for i in permutation_test]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.3- Functions to read, convert and resize the images and the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.utils import ReadImages, ReadMasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.4 - Define sets : train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files\n",
    "X_train_e8 = ReadImages(train_images_e8_files_rnd[:nb_train], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_train_e5 = ReadImages(train_images_e5_files_rnd[:nb_train], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_train_e2 = ReadImages(train_images_e2_files_rnd[:nb_train], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "y_train = ReadMasks(train_masks_files_rnd[:nb_train], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "\n",
    "X_val_e8 = ReadImages(train_images_e8_files_rnd[nb_train:], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_val_e5 = ReadImages(train_images_e5_files_rnd[nb_train:], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_val_e2 = ReadImages(train_images_e2_files_rnd[nb_train:], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "y_val = ReadMasks(train_masks_files_rnd[nb_train:], size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "\n",
    "X_test_e8 = ReadImages(test_images_e8_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_test_e5 = ReadImages(test_images_e5_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_test_e2 = ReadImages(test_images_e2_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "y_test = ReadMasks(test_masks_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "\n",
    "# 1 MRI  (e8)\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 1)\n",
    "X_train = X_train_e8\n",
    "X_val = X_val_e8\n",
    "X_test = X_test_e8\n",
    "\n",
    "# 3 MRI\n",
    "#input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "#X_train = tf.keras.layers.Concatenate()([X_train_e8, X_train_e5, X_train_e2])\n",
    "#X_val = tf.keras.layers.Concatenate()([X_val_e8, X_val_e5, X_val_e2])\n",
    "#X_test = tf.keras.layers.Concatenate()([X_test_e8, X_test_e5, X_test_e2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Shape train: \", X_train_e2.shape, X_train_e5.shape, X_train_e8.shape, y_train.shape)\n",
    "print(\" Shape valid: \", X_val_e2.shape, X_val_e5.shape, X_val_e8.shape, y_val.shape)\n",
    "print(\" Shape test : \", X_test_e2.shape, X_test_e5.shape, X_test_e8.shape, y_test.shape)\n",
    "\n",
    "print(\" Shape X_train and y_train : \", X_train.shape, y_train.shape)\n",
    "print(\" Shape X_val and y_val     : \", X_val.shape, y_val.shape)\n",
    "print(\" Shape X_test and y_test   : \", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "print(\" Type train: \", X_train_e2.dtype, X_train_e5.dtype, X_train_e8.dtype, y_train.dtype)\n",
    "print(\" Type valid: \", X_val_e2.dtype, X_val_e5.dtype, X_val_e8.dtype, y_val.dtype)\n",
    "print(\" Type test : \", X_test_e2.dtype, X_test_e5.dtype, X_test_e8.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.4- Plot images\n",
    "\n",
    "### 4.4.1 Display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.visualization import plot_overlay_segmentation, plot_compare_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Plot images with overlay (mask over original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_overlay_segmentation(X_train_e8[8:12], y_train[8:12])\n",
    "\n",
    "##plot_overlay_segmentation(X_train_e5[8:12], y_train[8:12])\n",
    "##plot_overlay_segmentation(X_train_e2[8:12], y_train[8:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.7- Prepare a generator for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:13.426908Z",
     "start_time": "2019-06-11T09:30:12.911908Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#        featurewise_center=True,\n",
    "#        featurewise_std_normalization=True,\n",
    "        rotation_range=10,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest',   # constant nearest, reflect or wrap\n",
    "        validation_split=0)\n",
    "\n",
    "class MyImageDataGenerator:\n",
    "    def __init__(self, X, y, data_generator, batch_size):\n",
    "        self.data_generator = data_generator\n",
    "        self.batch_size=batch_size\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.count = 0\n",
    "        self.permutation = np.random.permutation(self.X.shape[0])\n",
    "  \n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        self.permutation = np.random.permutation(self.X.shape[0])\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count > self.X.shape[0]//self.batch_size:\n",
    "            self.count = 0\n",
    "            self.permutation = np.random.permutation(self.X.shape[0])\n",
    " \n",
    "        max_index = self.count*self.batch_size+self.batch_size # valeur index de fin\n",
    "        if max_index > self.X.shape[0]-1:    # a t'on assez d'éléments ?\n",
    "            max_index = self.X.shape[0]-1\n",
    "        # permutation \n",
    "        X_rnd=np.asarray( [self.X[i] for i in self.permutation[self.count*self.batch_size:max_index]] )\n",
    "        y_rnd=np.asarray( [self.y[i] for i in self.permutation[self.count*self.batch_size:max_index]] )\n",
    "        \n",
    "        for i in range(0,X_rnd.shape[0]):\n",
    "            transf = self.data_generator.get_random_transform(img_shape = X_rnd.shape[1::] )   \n",
    "            X_rnd[i,:,:,:] = self.data_generator.apply_transform( X_rnd[i,:,:,:], transform_parameters = transf)\n",
    "            y_rnd[i,:,:,:] = self.data_generator.apply_transform( y_rnd[i,:,:,:], transform_parameters = transf)\n",
    "        self.count += 1\n",
    "        return X_rnd, y_rnd\n",
    " #       else:\n",
    "  #          raise StopIteration\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygenerator = MyImageDataGenerator(X_train, y_train, datagen, BATCH_SIZE)\n",
    "myiter = iter(mygenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(myiter)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an augmented data batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:14.195908Z",
     "start_time": "2019-06-11T09:30:13.428908Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if JUPYTER_DISPLAY_ON == True:\n",
    "    plot_overlay_segmentation(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **5- Network and training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 5.1- Initialize network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed functions and reset the TF graph (safer, if you want to tune the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.models import custom_unet, xception_unet, lowfeaturedecoder_unet\n",
    "from keras_unet.losses import dice_loss\n",
    "from keras_unet.hausdorff_loss import Weighted_Hausdorff_loss\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:14.510235Z",
     "start_time": "2019-06-11T09:30:14.197910Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" input_shape : \", input_shape)\n",
    "\n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model\n",
    "    \n",
    "model = custom_unet(\n",
    "    input_shape,\n",
    "    use_batch_norm=BATCHNORM_ON,  \n",
    "    num_classes=y_train[0].shape[-1],\n",
    "    filters=NBFILTERS_L1_UNET, \n",
    "    dropout=DROPOUT_RATE,\n",
    "    num_layers=NBLAYERS_UNET,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    cnn_activation=CNN_ACTIVATION,\n",
    "    output_activation='softmax',\n",
    "    kernel_regularizer=None # regularizers.l1(0.001)  # =None   for no regularization\n",
    "    )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:14.519910Z",
     "start_time": "2019-06-11T09:30:14.511921Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 5.2- Compile the model\n",
    "\n",
    "### Create callbacks for interactions during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:24.433203Z",
     "start_time": "2019-06-11T09:30:24.429202Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "print(\"model_filename : \", model_filename)\n",
    "\n",
    "\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    model_filename, \n",
    "    verbose=1, \n",
    "    monitor='val_dice_coef', #'val_loss'\n",
    "    mode ='max', # use 'min' or 'auto' if val_loss\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "callback_earlystopping = EarlyStopping(\n",
    "    monitor='val_dice_coef', #'val_loss'\n",
    "    mode ='max', # use 'min' or 'auto' if val_loss\n",
    "    patience=NBPATIENCE_EPOCHS,\n",
    "    restore_best_weights=True  # at the end of fitting, restore best model weights \n",
    ")\n",
    "\n",
    "logdir = \"./logs/scalars/\" + model_name + '_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%M'\")\n",
    "print(\"logdir : \", logdir)\n",
    "callback_tensorbooard = TensorBoard(\n",
    "    log_dir=logdir, profile_batch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "i.e. : Set optimization algorithm, loss, metrics, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:24.736240Z",
     "start_time": "2019-06-11T09:30:24.680251Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras_unet.metrics import dice_coef, iou, iou_thresholded\n",
    "from keras_unet.losses import dice_loss, adaptive_loss\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "# Optional scheduler to decrease learning rate according iterations or epoches\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#    initial_learning_rate,\n",
    "#    decay_steps=10000,\n",
    "#    decay_rate=0.96,\n",
    "#    staircase=True)\n",
    "\n",
    "model.compile(\n",
    "    loss=adaptive_loss,\n",
    "    #loss='categorical_crossentropy',   # <- other loss you can test\n",
    "    \n",
    "    optimizer=RMSprop(learning_rate=initial_learning_rate, rho=0.9, momentum=0.8, epsilon=1e-07, centered=True),\n",
    "    #optimizer=RMSprop(learning_rate=lr_schedule, rho=0.9, momentum=0.8, epsilon=1e-07, centered=True),     # <- other optimizer you can test\n",
    "    #optimizer=SGD(learning_rate=initial_learning_rate, momentum=0.99),  # 0.01 \n",
    "    #optimizer=Adam(learning_rate=initial_learning_rate),  # 0.0001\n",
    "    \n",
    "    metrics=[dice_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 5.3- Fit the model (train)\n",
    "\n",
    "Watch evolution of loss on epoches and time elapsed per epoch.\n",
    "If too long, you can stop the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-11T09:30:25.417Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,  # use the 'original' images \n",
    "#    myiter,           # use the images augmented by generator\n",
    "#    steps_per_epoch=NBSTEPS_PER_EPOCH,  # uncomment when using 'myiter' (a data generator)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NBEPOCHS,\n",
    "    validation_data=(X_val, y_val), \n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[callback_earlystopping, callback_checkpoint, callback_tensorbooard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 5.4- Plot training and validation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:22:16.614268Z",
     "start_time": "2019-04-25T14:22:16.240266Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_unet.visualization import plot_segm_history\n",
    "\n",
    "plot_segm_history(history,  metrics=['dice_coef', 'val_dice_coef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 5.5- Plot training and validation during training with Tensorboard (NOT ON SATURN CLOUD)\n",
    "\n",
    "You can use tensorboard to watch loss and metric evolutions during training.\n",
    "To do so you can :\n",
    "  1. start a terminal in jupyter lab (File -> New launcher then 'Terminal'\n",
    "  2. in this terminal execute : ```$ tensorboard  --logdir ./logs/scalars/  ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **6- Verify the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 6.1- Plot original + ground truth + pred + overlay (pred on top of original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val, batch_size=1, verbose=1) # GPU Size\n",
    "loss, dice_coef = model.evaluate(x=X_val, y=y_val, batch_size=1, verbose=1) # \n",
    "print(f\"loss : {loss}   Dice_coeff : {dice_coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "N_b = 0\n",
    "N_e = 10\n",
    "plot_compare_segmentation(X_val_e8[N_b:N_e], y_val[N_b:N_e], y_pred[N_b:N_e], \" \", spacing=(1,1), step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Questions\n",
    "\n",
    "- Run 20 more epoches and display curves and results again (... run the cells after §5.3). Are the results better ?\n",
    "- And 20 more ? \n",
    "- Train this network to have a DICE greater than 0.90. \n",
    "- Are the segmentation results convincing on the validation images ? Is this assessment correct?\n",
    "- Execute the next 2 cells : evaluation is performed on the test set ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 6.2- Inference on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=1, verbose=1) # GPU Size\n",
    "loss, dice_coef = model.evaluate(x=X_test, y=y_test, batch_size=1, verbose=1) # \n",
    "print(f\"loss : {loss}   Dice_coeff : {dice_coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "N_b = 0\n",
    "N_e = 10\n",
    "plot_compare_segmentation(X_test[N_b:N_e], y_test[N_b:N_e], y_pred[N_b:N_e], \" \", spacing=(1,1), step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Questions\n",
    "\n",
    "- Optimize the training paramters to improve segmentation results (according the validation set iou thresholded value): regularizations, data sets size, hyper parameters (you can enable regularizations (drop out, Batch Norm, L1 or L2 on weights). Note that you can't check all configurations, so plan tests with your collegues ! (and have a look at AutoML)\n",
    "    \n",
    "- Ok, the above training is long ;) So during training, display some information using Tensorboard: model shape and real time training curves.\n",
    "    \n",
    "- What is early stopping and what it should avoid? Discuss the evolution of losses and metrcis during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **7- Accessing the learned filters**\n",
    "    \n",
    "For understanding effect of layers, it can be of interest to display filters learned by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -> number of layers : \", len(model.layers) )\n",
    "for a in range(1,len(model.layers)):\n",
    "    print(f\" index : {a}, layer name : {model.layers[a].name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_l = 1 # Remember that some layers have no weights !\n",
    "print(\"-> Get config : \", model.layers[N_l].get_config())  # get whole information of the layer (config of the layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Get shape : \", model.layers[N_l].get_weights()[0].shape) #ok this line works only if some weights exist (X,Y,C,Neurons)\n",
    "w = model.layers[N_l].get_weights()[0][:,:,0,:]\n",
    "print(w.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = w.shape[2]\n",
    "nb_filters_per_line = 8   # \n",
    "if nb_filters > 256: nb_filters = 256\n",
    "if nb_filters < 9: nb_filters_per_line = nb_filters\n",
    "#assert nb_filters < 256\n",
    "plt.subplots(nb_filters//nb_filters_per_line,nb_filters_per_line,figsize=(15,10))\n",
    "for i in range(1,nb_filters+1,1):  \n",
    "    plt.subplot(nb_filters//nb_filters_per_line,nb_filters_per_line,i) # current plot is i\n",
    "    plt.imshow(w[:,:,i-1], interpolation='none', cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Question\n",
    "  \n",
    "- Visualize and discuss the filters of the first CNN after the first maxpooling (conv2d_3 layer). Justify its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **8- Observing the intermediate images**\n",
    "    \n",
    "Intermediate images are images at the output of layers.\n",
    "This task can be done efficiently using a second network that just copies all outputs of our network (the variable is *model*) to allow us displaying the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[1:]]  # skip input for fed problem\n",
    "print( len(layer_outputs) )\n",
    "# Extracts the outputs \n",
    "activation_model  = models.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select an image (in the validation set... it can be changed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_img_to_display = 9\n",
    "print(X_val_e8[Index_img_to_display:Index_img_to_display+1,:,:,:].shape)\n",
    "activations = activation_model.predict(X_val_e8[Index_img_to_display:Index_img_to_display+1,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select an activation output. For convenience, indexes of layers have been preserved from 0 to N (0 means input and is not mapped to this model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_Output_Layer = N_l\n",
    "first_layer_activation = activations[Index_Output_Layer - 1]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(first_layer_activation[0, :, :, 2], cmap='viridis')  \n",
    "\n",
    "nb_filters =first_layer_activation.shape[3]\n",
    "nb_filters_per_line = 8   # \n",
    "if nb_filters > 256: nb_filters = 256\n",
    "if nb_filters < 9: nb_filters_per_line = nb_filters\n",
    "#assert nb_filters < 256\n",
    "plt.subplots(nb_filters//nb_filters_per_line,nb_filters_per_line,figsize=(15,10))\n",
    "for i in range(1,nb_filters+1,1):  \n",
    "    plt.subplot(nb_filters//nb_filters_per_line,nb_filters_per_line,i) # current plot is i\n",
    "    plt.imshow(first_layer_activation[0, :, :, i-1], interpolation='bilinear', cmap='viridis')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Questions\n",
    "  \n",
    "- Visualize and discuss the filters conv2d_3 layer (the first CNN after the first maxpooling). Justify its shape.\n",
    "- Visualize and discuss the last filter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Questions, to finish ... \n",
    " \n",
    "- Modify the network to use the 3 MRI sequences as input (e8, e5 and e2). Follow the training with tensorboard. Is DICE improved?\n",
    "\n",
    "- Modify the code to use the Xception_unet with ADAM optimizer. First change the __model_filename__ string to facilitate comparisons with previous training.\n",
    " \n",
    "- How much time to manual segment an image ? Why it is critical to have good (expert, consistent) annotated images? With no more new data, what can be done to __augment__ our train data set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard (reminder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary on floydhub\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# tensorboard --host 134.214.236.10 --logdir ./logs/scalars/                ##  --port --bind_all\n",
    "\n",
    "# http://134.214.236.10:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hints for usage of 3 MRI as input\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3) # see the \"3\" instead of 1\n",
    "\n",
    "concat_train = tf.keras.layers.Concatenate()([X_train_e8, X_train_e5, X_train_e2])\n",
    "concat_val = tf.keras.layers.Concatenate()([X_val_e8, X_val_e5, X_val_e2])\n",
    "print(concat_train.shape)\n",
    "plt.imshow(concat_train[0]) # display in color :p\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    concat_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NBEPOCHS,\n",
    "    validation_data=(concat_val, y_val),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[ callback_earlystopping, callback_checkpoint, callback_tensorbooard]\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model.predict(concat_val, batch_size=1, verbose=1) # GPU Size\n",
    "loss, dice_coef = model.evaluate(x=concat_val, y=y_val, batch_size=1, verbose=1) # \n",
    "print(f\"loss : {loss}   Dice_coeff : {dice_coef}\")\n",
    "print(y_pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hints for Xception Unet\n",
    "\n",
    "model = xception_unet(\n",
    "    input_shape,\n",
    "    num_classes=y_train[0].shape[-1],\n",
    "    filters=NBFILTERS_L1_UNET, \n",
    "    output_activation='softmax'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
