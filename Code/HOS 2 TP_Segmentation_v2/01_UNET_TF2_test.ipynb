{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNet Segmentation TEST (inference)**\n",
    "### *Keras / Tensorflow 2*\n",
    "\n",
    "*thomas.grenier@creatis.insa-lyon.fr*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load a pre-trained model based on UNet architecture that allows image segmentation.\n",
    "UNet was initially proposed by Ronneberger in 2015 and is now the most used architecture for semantic image segmentation.\n",
    "\n",
    "### <span style=\"color:brown\"> **What is segmentation ?**\n",
    "\n",
    "Segmentation is the process of assigning a label (that represents a class or an object) to a pixel.\n",
    "\n",
    "If there is only one kind of object, segmentation is said to be a binary segmentation (ie object and backgrouond).\n",
    "For more classes, segmentation is said multi-class segmenation.\n",
    "\n",
    "The next figure illustrated available MRI images (WIP mDixon axial BW036 2nsa sshot HR2 TE2_3 FFE CLEAR, echo 2, 5 and 8) of shoulder from Geneva Hospital with the manual segmentation of 5 muscles done by experts (Jean-Baptiste Pialat, Lexane Rocle, Jeff Delorme):\n",
    "\n",
    "<img src=\"figures/P02_71_e2_crop.png\" alt=\"Segmentation\" style=\"width: 22%;\"/> <img src=\"figures/P02_71_e5_crop.png\" alt=\"Segmentation\" style=\"width: 22%;\"/> <img src=\"figures/P02_71_e8_crop.png\" alt=\"Segmentation\" style=\"width: 22%;\"/> <img src=\"figures/P02_71_crop.png\" alt=\"Segmentation\" style=\"width: 22%;\"/>\n",
    "    \n",
    "\n",
    "In the following, we are going to segment in 2D these 5 muscles on such MRI (mainly using echo 8 images).\n",
    "\n",
    "### <span style=\"color:brown\"> **What is the difference between semantic segmentation and instance segmentation ?**\n",
    "\n",
    "If several objects of the same class are present in an image, semantic segmentation assigns the same class to all these objects.\n",
    "Instance segmentation will also dissociate each of these objects and allows a direct count of them.\n",
    "One can note that the step of dissociation (or counting) can be done by classical image processing methods after a semantic segmentation.\n",
    "\n",
    "<img src=\"figures/SemanticIntanceSegmentation.png\" alt=\"Semantic vs Instance Segmentation\" style=\"width: 75%;\"/>    \n",
    "\n",
    "**Here we focus on multi-class semantic segmentation** using U-Net deep neural network.\n",
    "\n",
    "### <span style=\"color:brown\"> **Unet in (very) brief**\n",
    "This Network is fully **convolutional**. \n",
    "It uses skip connections from the encoder side to the decoder side to preserve scale information.\n",
    "\n",
    "<img src=\"figures/UNet.png\" alt=\"UNet\" style=\"width: 70%;\"/>\n",
    "\n",
    "In this notebook, we start by using a trained network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Question : </span> On the last convolution layer, how many filters are needed to segment our 5 muscles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **1- System setting**\n",
    "\n",
    "We start by loading necessary libraries and setting variables.\n",
    "Some of them are related to data.\n",
    "But, to ensure that the code works on almost all infrastructures, low-level system variable are also initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medpy\n",
    "!pip install tqdm\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:08.991851Z",
     "start_time": "2019-06-11T09:30:08.985860Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model    \n",
    "    \n",
    "# select the device (CPU or GPU) to run on\n",
    "num_CPU = 1\n",
    "num_cores = 4\n",
    "\n",
    "# KERNEL msut be restarted if you change GPU 0 -> 1 or 1 -> 0 (cannot change runtime after initialization)\n",
    "GPU = 0  # GPU = 0 : CPU Only ; GPU = 1 : use GPU\n",
    "\n",
    "physical_gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "physical_cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "print(physical_gpu_devices)\n",
    "print(physical_cpu_devices)\n",
    "\n",
    "if GPU:\n",
    "    tf.config.set_visible_devices(physical_gpu_devices[0], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    print(visible_devices)  \n",
    "    for device in visible_devices:\n",
    "        print(device)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_CPU)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(num_cores) \n",
    "else:\n",
    "    try:\n",
    "      # Disable all GPUS\n",
    "      tf.config.set_visible_devices([], 'GPU')\n",
    "      visible_devices = tf.config.get_visible_devices()\n",
    "      for device in visible_devices:\n",
    "        print(device)\n",
    "        assert device.device_type != 'GPU'\n",
    "    except:\n",
    "      # Invalid device or cannot modify virtual devices once initialized.\n",
    "      pass\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_CPU)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **2- Load and Prepare images and model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    \n",
    "> **Before running cells of 2.1**, you have to   \n",
    ">   1. if not in the current directory, download the file ```dlss21_ho4_data.tar.gz```  ( can be done on linux bash with ```$ wget https://gitlab.in2p3.fr/thomas.grenier/tp4ss_segmentation/-/raw/master/dlss21_ho4_data.tar.gz```)\n",
    ">   2. un tar the downloaded file in the directory of this notebook (to do so on bash : ```$ tar xzf dlss21_ho4_data.tar.gz```).\n",
    "\n",
    "The next cell can do it for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following commands work on linux system and download the file\n",
    "!rm dlss21_ho4_data.tar.gz\n",
    "!wget https://gitlab.in2p3.fr/thomas.grenier/tp4ss_segmentation/-/raw/master/dlss21_ho4_data.tar.gz\n",
    "!tar fax dlss21_ho4_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    \n",
    "> **Before running the cells of 2.1**, you have to download the **MODEL** ```dlss21_ho4_model```:\n",
    ">    1.  link : https://www.creatis.insa-lyon.fr/~grenier/wp-content/uploads/teaching/DeepLearning/Unet_f32_b16_l5_do0.1_Std_BN_input96.h5\n",
    ">    2.  copy in the directory than this notebook.\n",
    "    \n",
    "The next cell can do it for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command works on linux and download the model\n",
    "!wget https://www.creatis.insa-lyon.fr/~grenier/wp-content/uploads/teaching/DeepLearning/Unet_f32_b16_l5_do0.1_Std_BN_input96.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 2.1- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.utils import ReadImages, ReadMasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:09.006881Z",
     "start_time": "2019-06-11T09:30:08.994853Z"
    }
   },
   "outputs": [],
   "source": [
    "# test \n",
    "test_masks_files     = glob.glob(\"./dlss21_ho4_data/test/labels/*.png\")\n",
    "test_images_e8_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e8.png\")\n",
    "test_images_e5_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e5.png\")\n",
    "test_images_e2_files = glob.glob(\"./dlss21_ho4_data/test/images/*_e2.png\")\n",
    "\n",
    "# Data related values\n",
    "IMG_SIZE = 96\n",
    "\n",
    "JUPYTER_DISPLAY_ON = True\n",
    "\n",
    "model_filename = './Unet_f32_b16_l5_do0.1_Std_BN_input96.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:09.224878Z",
     "start_time": "2019-06-11T09:30:09.009844Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images_e2_files.sort()\n",
    "test_images_e5_files.sort()\n",
    "test_images_e8_files.sort()\n",
    "test_masks_files.sort()\n",
    "\n",
    "print( \" testing   :  \", len(test_images_e2_files), len(test_images_e5_files), len(test_images_e8_files), len(test_masks_files) )\n",
    "\n",
    "permutation_test = np.random.permutation( len(test_masks_files))\n",
    "test_images_e2_files_rnd=[test_images_e2_files[i] for i in permutation_test]\n",
    "test_images_e5_files_rnd=[test_images_e5_files[i] for i in permutation_test]\n",
    "test_images_e8_files_rnd=[test_images_e8_files[i] for i in permutation_test]\n",
    "test_masks_files_rnd=[test_masks_files[i] for i in permutation_test]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files\n",
    "X_test_e8 = ReadImages(test_images_e8_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_test_e5 = ReadImages(test_images_e5_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "X_test_e2 = ReadImages(test_images_e2_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "y_test = ReadMasks(test_masks_files_rnd, size=(IMG_SIZE, IMG_SIZE), crop=(30,30,330,330))\n",
    "\n",
    "# 1 MRI\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 1)\n",
    "X_test = X_test_e8\n",
    "\n",
    "# 3 MRI\n",
    "#input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "#X_test = tf.keras.layers.Concatenate()([X_test_e8, X_test_e5, X_test_e2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:09.232851Z",
     "start_time": "2019-06-11T09:30:09.227865Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" Shape test : \", X_test_e2.shape, X_test_e5.shape, X_test_e8.shape, y_test.shape)\n",
    "print(\" Shape X_test and y_test : \", X_test.shape,  y_test.shape)\n",
    "print(\" Type test : \", X_test_e2.dtype, X_test_e5.dtype, X_test_e8.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images_e8_files_rnd[3])\n",
    "print(test_images_e8_files_rnd[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Question : </span> Observe the min and max of data. Why range is so critical ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 2.2- Plot images + masks + overlay (mask over original)\n",
    "\n",
    "Check, check and re-check again your data. \n",
    "Many mistakes come from data...\n",
    "\n",
    "**The following cell displays images, manual annotations and an overlays for ease of visualization**\n",
    "Nothing done by the network yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.visualization import plot_overlay_segmentation, plot_compare_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot images with overlay (mask over original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overlay_segmentation(X_test_e8[3:15], y_test[3:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **3- Load the network and its weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T09:30:14.510235Z",
     "start_time": "2019-06-11T09:30:14.197910Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.losses import dice_loss, dice_coef, adaptive_loss\n",
    "from tensorflow.keras import models\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# load the network with its custom functions\n",
    "loaded_model = models.load_model(model_filename, custom_objects={'dice_coef': dice_coef, 'adaptive_loss': adaptive_loss, 'dice_loss': dice_loss})\n",
    "\n",
    "# display the network\n",
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **4- Predict segmentations on the whole test set using the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_test, batch_size=1, verbose=1) # GPU Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dice_coef = loaded_model.evaluate(x=X_test, y=y_test, batch_size=1, verbose=1) # \n",
    "print(f\"loss : {loss}   dice_coeff : {dice_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.1- Plot images : MRI with overlay of ground truth + MRI with prediction overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:24:19.609395Z",
     "start_time": "2019-04-25T14:24:03.016351Z"
    }
   },
   "outputs": [],
   "source": [
    "N_b = 0\n",
    "N_e = 10\n",
    "plot_compare_segmentation(X_test_e8[N_b:N_e], y_test[N_b:N_e], y_pred[N_b:N_e], \" \", spacing=(1,1), step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Question : </span>\n",
    "\n",
    "- How the quality of this network is assessed ? what is Dice ?\n",
    "- Visually consider images and the predictions, what is your qualitative assessment ?\n",
    "\n",
    "\n",
    "### <span style=\"color:red\"> Question : </span> Importance of 'device'.\n",
    "- How long was the prediction time (or evaluation time)?\n",
    "- This step ran on CPU. Now, change the GPU variable (1 -> 0) in the first cell of code and restart the kernel. Then run again the notebook. Compare the evaluate time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> 4.2- Evalaution with DICE/Hausdorff distance and Average symmetric surface distance\n",
    "    \n",
    "First, load the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.evaluation import  evaluate_segmentation, evaluate_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then perform a unique evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras_unet import evaluation\n",
    "dice, hausdorff, assds = evaluate_segmentation(y_test[1], y_pred[1], voxel_spacing = [1, 1])\n",
    "print(\"Dice:\", dice)\n",
    "print(\"hausdorff:\", hausdorff)\n",
    "print(\"assds\",assds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on the whole test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_all, hausdorff_all, assd_all, valid_all = evaluate_set(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dice_all\", dice_all)\n",
    "print(\"hausdorff\", hausdorff_all)\n",
    "print(\"assd\", assd_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the rendering for your presentation ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML \n",
    "\n",
    "overall_results = np.column_stack((dice_all, hausdorff_all, assd_all))\n",
    "#print(overall_results)\n",
    "\n",
    "# Graft our results matrix into pandas data frames \n",
    "overall_results_df = pd.DataFrame(data=overall_results, index = [\"All\", \"1\", \"2\", \"3\", \"4\", \"5\"], \n",
    "                                  columns=[\"Dice\", \"Hausdorff\", \"ASSD\"]) \n",
    "\n",
    "# Display the data as HTML tables and graphs\n",
    "display(HTML(overall_results_df.to_html(float_format=lambda x: '%.3f' % x)))\n",
    "overall_results_df.plot(kind='bar', figsize=(10,6)).legend() #bbox_to_anchor=(1.6,0.9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to your paper (latex) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(overall_results_df.to_latex(float_format=lambda x: '%.3f' % x)) # column_format='cccc'\n",
    "\n",
    "# max Dice value in blod:\n",
    "latex_tab = overall_results_df.style.highlight_max( props='textbf:--rwrap;', subset=[\"Dice\"])\n",
    "       \n",
    "# min \"Hausdorff\", \"ASSD\" in bold\n",
    "latex_tab.highlight_min( props='textbf:--rwrap;', subset=[\"Hausdorff\", \"ASSD\"])\n",
    "\n",
    "#limit decimal do different precision\n",
    "latex_tab.format({\n",
    "   \"Dice\": '{:.2f}',\n",
    "   \"Hausdorff\": '{:.1f}',\n",
    "   \"ASSD\": '{:.3f}'\n",
    "})  \n",
    "\n",
    "# generate latex code\n",
    "print( latex_tab.to_latex(\n",
    "    column_format=\"cccc\", position=\"h\", position_float=\"centering\",\n",
    "    hrules=True, label=\"table:SegmentationResults\", caption=\"Averaged DSC, HD and ASSD on the test set\",\n",
    "    multirow_align=\"t\", multicol_align=\"r\")  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\"> **Now :  1- Shutdown the Kernel**\n",
    "    \n",
    "    (menu --> Kernel --> \" Shut down kernel...\" )\n",
    "    \n",
    "    This will stop the kernel and free the ressources allocated (GPU memory...)\n",
    "    \n",
    "<span style=\"color:brown\"> **Then : 2-  go to 2_UNET_TF2_train notebook to train your UNet!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
