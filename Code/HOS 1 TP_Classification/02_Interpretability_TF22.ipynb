{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Hands-on Deep Learning â€“ Interpretability of Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This hands-on was created by Thomas Grenier, CREATIS_\n",
    "\n",
    "thomas.grenier@creatis.insa-lyon.fr,\n",
    "michael.sdika@creatis.insa-lyon.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Introduction on data**\n",
    "\n",
    "This hands-on is computer scientist-oriented more than application-oriented. \n",
    "\n",
    "It is the second part of the classification hands-on: we will use the same data and study GradCam.\n",
    "First cells are exactly the same as the first notebook.\n",
    "Changes occur at section \"E\"\n",
    "\n",
    "> Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **A - Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1 - First, import a few common modules, ensure MatplotLib plots figures inline, and functions for converting data and displaying reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "!pip install scikit-learn\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "# if trouble with libGL consider to execute this line in a shell: apt-get install python3-opencv\n",
    "# and execute this line : !pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2- We import TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**if no error occurs, your working environment is ok and you can go to next part,\n",
    "> else ... call an assistant for help!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **B - Read the data**\n",
    "\n",
    "<span style=\"color:red\">\n",
    "    \n",
    "\n",
    "We first download the data and then deflate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm dlss21_ho1_data.tar.gz\n",
    "#!wget \"https://gitlab.in2p3.fr/thomas.grenier/tp1ss_classification/-/raw/master/dlss21_ho1_data.tar.gz\"\n",
    "#!tar xzf dlss21_ho1_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B1 - Open files and convert to numpy / tensorflow** \n",
    "\n",
    "> The next cells will open all images and create three datasets containing the images and the corresponding labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important values and initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_height = 64\n",
    "desired_width  = 64\n",
    "\n",
    "input_train_path = os.getcwd()+'/dlss21_ho1_data/train'           \n",
    "input_test_path = os.getcwd()+'/dlss21_ho1_data/test'            \n",
    "print(f\"input train path : {input_train_path}\")\n",
    "print(f\"input test path  : {input_test_path}\")\n",
    "\n",
    "class_names = ['PD-A', 'PD-C', 'PD-S', 'T1-A', 'T1-C', 'T1-S', 'T2-A', 'T2-C', 'T2-S']\n",
    "\n",
    "input_shape = (desired_height,desired_width,1)\n",
    "input_shape_rgb = (desired_height,desired_width,3)\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first list files for the train and validation sets.\n",
    "train_val_image_file_list = []\n",
    "train_val_label_list = []\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    for filename in glob.iglob( os.path.join(input_train_path,class_names[i]) + '/**/*.png', recursive=True):\n",
    "        # extract patient number and slice\n",
    "        train_val_image_file_list.append(filename)\n",
    "        train_val_label_list.append( i )     \n",
    "\n",
    "print(len(train_val_image_file_list), len(train_val_label_list))\n",
    "print(train_val_image_file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permutation\n",
    "permutation = np.random.permutation( len(train_val_label_list) )\n",
    "train_val_images_files=[train_val_image_file_list[i] for i in permutation]\n",
    "train_val_labels=[train_val_label_list[i] for i in permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the train set in two parts : train and validation\n",
    "VALIDATION_RATIO = 0.2\n",
    "nb_train = int( len(train_val_label_list) * (1 - VALIDATION_RATIO) )\n",
    "# list images for both datasets\n",
    "train_images_files = train_val_images_files[:nb_train]\n",
    "train_labels =train_val_labels[:nb_train]\n",
    "\n",
    "val_images_files = train_val_images_files[nb_train:]\n",
    "val_labels = train_val_labels[nb_train:]\n",
    "\n",
    "# for labels also convert them to numpy\n",
    "y_train = np.asarray(train_labels)\n",
    "y_val = np.asarray(val_labels)\n",
    "\n",
    "print( train_val_images_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading images \n",
    "def ReadImages(images_files):\n",
    "    X = []\n",
    "    for index in tqdm(range(len(images_files))):\n",
    "        image_read = cv2.imread(images_files[index], cv2.IMREAD_COLOR)\n",
    "        image_read = cv2.resize(image_read, dsize = (desired_width, desired_height), interpolation = cv2.INTER_LINEAR)\n",
    "        X.append(image_read)\n",
    "    X = np.asarray(X, dtype=np.uint8)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "X_train_rgb = ReadImages(train_images_files)\n",
    "X_val_rgb = ReadImages(val_images_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B2 - Check the output data size and type**\n",
    "We recommend to check as often as needed the shape and type of data.\n",
    "\n",
    "Don't go further if you can't see what is inside array and tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information\n",
    "print(\" Shape : \" , X_train_rgb.shape, X_val_rgb.shape)\n",
    "print(\" Type  : %s  %s\"%(X_train_rgb.dtype, X_val_rgb.dtype))\n",
    "print(\" Max   : %d  %d\"%(X_train_rgb.max(), X_val_rgb.max()))\n",
    "print(\" Min   : %d  %d\"%(X_train_rgb.min(), X_val_rgb.min()))\n",
    "\n",
    "print(\" Labels Shape : \" , y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B3 - Displaying images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, aspect=\"equal\", cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_x = 5\n",
    "nb_y = 5\n",
    "plt.figure(figsize=(15,15*nb_x/nb_y), dpi=100)\n",
    "for i in range(1, nb_x * nb_y +1):\n",
    "    plt.subplot(nb_x, nb_y ,i)\n",
    "    plot_image( X_train_rgb[i-1] )\n",
    "    plt.title(class_names[y_train[i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B4 - Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing same for test set\n",
    "test_image_file_list = []\n",
    "test_label_list = []\n",
    "           \n",
    "for i in range(len(class_names)):\n",
    "    for filename in glob.iglob( os.path.join(input_test_path,class_names[i]) + '/**/*.png', recursive=True):\n",
    "        # extract patient number and slice\n",
    "        test_image_file_list.append(filename)\n",
    "        test_label_list.append( i )            \n",
    "\n",
    "print(len(test_image_file_list), len(test_label_list))\n",
    "\n",
    "X_test_rgb = ReadImages(test_image_file_list)\n",
    "y_test = np.asarray(test_label_list)\n",
    "\n",
    "print(\" Shape : \" , X_test_rgb.shape, y_test.shape)\n",
    "print(\" Type  : %s  %s\"%(X_test_rgb.dtype, y_test.dtype))\n",
    "print(\" Max   : %d  %d\"%(X_test_rgb.max(), y_test.max()))\n",
    "print(\" Min   : %d  %d\"%(X_test_rgb.min(), y_test.min()))\n",
    "\n",
    "nb_x = 5\n",
    "nb_y = 5\n",
    "plt.figure(figsize=(15,15*nb_x/nb_y), dpi=100)\n",
    "for i in range(1, nb_x * nb_y +1):\n",
    "    plt.subplot(nb_x, nb_y ,i)\n",
    "    plot_image( X_test_rgb[i-1] )\n",
    "    plt.title(class_names[y_test[i-1]])\n",
    "# here we have not shuffle filenames :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify shapes that should look like ( batch_size, desired_height, desired_width, 3 ) and (batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B5 - Convert the data shape and type**\n",
    "The _png_ files are RGB, we convert them to gray images for each set (train, validation and test) in order to be compatible with all tested approaches.\n",
    "\n",
    "We also divide intensity values by 255 to bound range in [0;1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the free data set : rgb to gray scale images, then normalization\n",
    "X_train = X_train_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_train = np.expand_dims(X_train,-1)\n",
    "\n",
    "X_val = X_val_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_val = np.expand_dims(X_val,-1)\n",
    "\n",
    "X_test = X_test_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_test = np.expand_dims(X_test,-1)\n",
    "\n",
    "print(\" Shape : \", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\" Type  : %s  %s %s\"%(X_train.dtype, X_val.dtype, X_test.dtype))\n",
    "print(\" Max   : %d %d %d\"%(X_train.max(), X_val.max(), X_test.max()) )\n",
    "print(\" Min   : %d %d %d\"%(X_train.min(),X_val.min(), X_test.min()) )\n",
    "plot_image( X_test[0].squeeze() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**All done, go to the classification step!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **C - Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for displaying reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example : ClassificationReport(model, X_test, y_test, class_names)\n",
    "def ClassificationReport(model, X_test, y_test, labels_name):  \n",
    "    # predict \n",
    "    print(\"Prediction on test images\")\n",
    "    predicted = model.predict(X_test)\n",
    "    y_pred = np.argmax(predicted, axis=-1)\n",
    "    \n",
    "    print(\"Classification report :\\n%s\\n\"\n",
    "         % (metrics.classification_report(y_test, y_pred)) )\n",
    "    cm = metrics.confusion_matrix( y_test, y_pred )\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=labels_name)\n",
    "    disp.plot()\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **D - CNN Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **D2 - Convolutional Neural Network (CNN)**\n",
    "\n",
    "In order to try to improve the results, we are now looking at convolutional neural networks, of which an example architecture is provided below.\n",
    "\n",
    "<img src=\"./figures/architecture_cnn_en.png\" alt=\"Architecture CNN\" style=\"width: 90%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of variables that you will have to play with during the session to study their influence\n",
    "\n",
    "# Variables related to the FCN architecture\n",
    "nb_mlp_neurons = 16    # Number of neurons for one layer\n",
    "nb_cnn_neurons = 16    # number of feature maps\n",
    "\n",
    "# Variables related to the optimization process\n",
    "batch_size  = 16       # Number of images for each batch during the training process \n",
    "nb_epochs   = 15       # number of epochs used during the training process\n",
    "learning_rate = 0.001  # learning rate used during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a CNN (Convolutional Neural Network) model using a sequential declaration allowed by the tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code allows you to free up memory space \n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model\n",
    "    \n",
    "# Define a CNN model from sequential declaration    \n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons//2, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons*2, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(nb_mlp_neurons, activation='relu'),  #128\n",
    "  tf.keras.layers.Dense(num_classes, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "# Display the network structure on the console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile (define the optimization scheme) the model to be ready for training\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  validation_data=(X_val, y_val),\n",
    "  batch_size=batch_size,  \n",
    "  epochs=nb_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to evaluate the performance of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall accuracy of the model on the test dataset \n",
    "test_loss, test_accuray = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', test_accuray)\n",
    "\n",
    "# Compute and display standart classication metrics, i.e. precision, recall, f1-score, support and confusion matrix \n",
    "ClassificationReport(model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **E - Why it works so good?**\n",
    "\n",
    "Can we analyze how the network analyze the images? \n",
    "\n",
    "In some way the answer is yes, partially. This is the aim of __explainability__ and __interpretability__.\n",
    "    \n",
    " \n",
    "The idea is to observe which part of the image influence the final classification decision.\n",
    "For a given class and a given image, which pixels contributes to activate the last layer?\n",
    "Many methods have been proposed (GradCAM, intergrated CAM , see the work of Linardatos 2021 https://www.mdpi.com/1099-4300/23/1/18/pdf ).\n",
    "    \n",
    "Here we propose to use the GradCam approach to explore which pixels contributes to the final class activation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.input], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "    \n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function to display the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def display_gradcam(img_array, heatmap, alpha=0.4):\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    img_array = np.uint8(255 * img_array)\n",
    "    \n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img_array.shape[2], img_array.shape[1]))\n",
    "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img_array[0]\n",
    "    return np.float32(superimposed_img/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **E2 - Exploring activation maps**\n",
    "    \n",
    "Now explore the activation map of a given image and the last convolutional layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'conv2d_2'    # <----- selected layer (simple network)\n",
    "img_array = X_test[1:2]               # <----- selected image\n",
    "print(img_array.shape)\n",
    "\n",
    "# Remove the last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "\n",
    "print(\"Predicted outputs:\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"arg max of Predicted:\", np.argmax(preds))\n",
    "index_pred = np.argmax(preds)      # <----- select the index you want to analyse how it has been activated\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, last_conv_layer_name, index_pred )\n",
    "\n",
    "print(heatmap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_img = display_gradcam(img_array, heatmap)\n",
    "plt.imshow(active_img, aspect=\"equal\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- When index_pred is the arg max of the predictions, where are the pixels that activate this class? What did you expect?\n",
    "- And when index_pred is not the arg max of the predictions (is such case, the activation means: why the network does not conclude this is this 'wrong' class), where are the pixels that activate this class?    \n",
    "- Conclusions? Why the network work so well ?\n",
    "- Can you propose a way to remove this biais from the inputs images?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
