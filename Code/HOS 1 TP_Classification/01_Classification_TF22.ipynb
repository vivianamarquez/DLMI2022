{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Hands-on Deep Learning â€“ Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This hands-on was originally created by Thomas Grenier (TensorFlow) and Fabien Millioz (PyTorch), CREATIS.\n",
    "\n",
    "thomas.grenier@creatis.insa-lyon.fr,\n",
    "michael.sdika@creatis.insa-lyon.fr,\n",
    "olivier.bernard@creatis.insa-lyon.fr,\n",
    "odyssee.merveille@creatis.insa-lyon.fr,\n",
    "fabien.millioz@creatis.insa-lyon.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Introduction on data**\n",
    "\n",
    "This hands-on is computer scientist-oriented more than application-oriented.\n",
    "\n",
    "And so, this practice focuses on a meaningless toy example inspired from the MNIST manuscript numbers classification challenge that is considered as the 'hello world' example for some machine learning and almost all neural networks.\n",
    "\n",
    "MR images come from [IXI Dataset](https://brain-development.org/ixi-dataset/)\n",
    "\n",
    "Here, we have to recognize whether a brain slice image is axial, sagittal or coronal and comes from MRI T1w, MRI T2w and Proton Density MR (PD). So, this is a classification problem with 9 classes, as follows:\n",
    "   \n",
    "<img src=\"./figures/Figure1_SagittalAxialCoronal_small.png\" alt=\"SagittalAxialCoronal\" style=\"width: 70%\"/>\n",
    "\n",
    "Amount of images of each class for train and test datasets is :  \n",
    "- Total images for training:\n",
    "   - PD-A (PD Axial)    : 2701\n",
    "   - PD-C (PD Coronal)  : 2723\n",
    "   - PD-S (PD Sagittal) : 2698\n",
    "   - T1-A (T1 Axial)    : 2715\n",
    "   - T1-C (T1 Coronal)  : 2758\n",
    "   - T1-S (T1 Sagittal) : 2711\n",
    "   - T2-A (T2 Axial)    : 2696\n",
    "   - T2-C (T2 Coronal)  : 2717\n",
    "   - T2-S (T2 Sagittal) : 2699\n",
    "- Total images for testing:\n",
    "   - PD-A (PD Axial)     : 689\n",
    "   - PD-C (PD Coronal)   : 696\n",
    "   - PD-S (PD Sagittal)  : 693\n",
    "   - T1-A (T1 Axial)     : 679\n",
    "   - T1-C (T1 Coronal)   : 685\n",
    "   - T1-S (T1 Sagittal)  : 676\n",
    "   - T2-A (T2 Axial)     : 684\n",
    "   - T2-C (T2 Coronal)   : 690\n",
    "   - T2-S (T2 Sagittal)  : 686\n",
    "\n",
    "We have enough data to test classification approaches such as random forest, MLP and CNN.\n",
    "\n",
    "> Let's go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **A - Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1 - First, import a few common modules, ensure MatplotLib plots figures inline, and functions for converting data and displaying reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "!pip install scikit-learn\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "# if trouble with libGL consider to execute this line in a shell: apt-get install python3-opencv\n",
    "# !pip install opencv-python\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2- We import TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**if no error occurs, your working environment is ok and you can go to next part,\n",
    "> else ... call an assistant for help!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **B - Read the data**\n",
    "\n",
    "<span style=\"color:red\">\n",
    "    \n",
    "\n",
    "We first download the data and then deflate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dlss21_ho1_data.tar.gz\n",
    "!wget \"https://gitlab.in2p3.fr/thomas.grenier/tp1ss_classification/-/raw/master/dlss21_ho1_data.tar.gz\"\n",
    "#!wget \"https://www.creatis.insa-lyon.fr/~grenier/wp-content/uploads/teaching/DeepLearning/dlss21_ho1_data.tar.gz\"\n",
    "!tar xzf dlss21_ho1_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B1 - Open files and convert to numpy / tensorflow** \n",
    "\n",
    "Now start by reading _png_ files stored in the data train and test directories. \n",
    "Note that the filename of the _png_ image describes which view (Axial, Coronal, Sagital) and modality (T1w, T2w and PD) correspond to the image.\n",
    "\n",
    "These _png_ correspond to slices extracted from 3D volumes of the [IXI dataset](https://brain-development.org/ixi-dataset/). In order to reduce the data variability (and simplify the slice extraction scripts), all MRI volumes were registered on a reference image using [elastix](http://elastix.isi.uu.nl/) with an affine transform. \n",
    "\n",
    "Thus, origin, spacing, orientation and number of pixels per dimension of all volumes are the same and therefore will not need to be taken into account systematically by the learning scripts (using SimpleITK, MedPy, nibabel...).\n",
    "\n",
    "Then, slices are extracted and the window/level was adapted (using [ITK](https://itk.org/)).\n",
    "\n",
    "Finally, they were resized then padded/cropped to 64x64 using [ImageMagick](https://www.imagemagick.org/).\n",
    "\n",
    "Image files in __dlss21_ho1_data/train/__  are organized as follow:\n",
    "  - dlss21_ho1_data/train/PD-A/002_116_PD-A.png\n",
    "                              /002_42_PD-A.png\n",
    "                              /002_44_PD-A.png\n",
    "                              /002_53_PD-A.png\n",
    "                               ...\n",
    "  - dlss21_ho1_data/train/PD-C/002_152_PD-C.png\n",
    "                              /002_175_PD-C.png\n",
    "                              /002_177_PD-C.png\n",
    "                              /002_198_PD-C.png\n",
    "                               ...\n",
    "  - dlss21_ho1_data/train/PD-S/...\n",
    "  - dlss21_ho1_data/train/T1-A/...\n",
    "  - dlss21_ho1_data/train/T1-C/...\n",
    "  - dlss21_ho1_data/train/T1-S/...\n",
    "  - dlss21_ho1_data/train/T2-A/...\n",
    "  - dlss21_ho1_data/train/T2-C/...\n",
    "  - dlss21_ho1_data/train/T2-S/...\n",
    "\n",
    "And the same for __dlss21_ho1_data/test/__ with different subjects than ones present in the train set.\n",
    "\n",
    "\n",
    "> The next cells will open all images and create three datasets containing the images and the corresponding labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important values and initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_height = 64\n",
    "desired_width  = 64\n",
    "\n",
    "input_train_path = os.getcwd()+'/dlss21_ho1_data/train'           \n",
    "input_test_path = os.getcwd()+'/dlss21_ho1_data/test'            \n",
    "print(f\"input train path : {input_train_path}\")\n",
    "print(f\"input test path  : {input_test_path}\")\n",
    "\n",
    "class_names = ['PD-A', 'PD-C', 'PD-S', 'T1-A', 'T1-C', 'T1-S', 'T2-A', 'T2-C', 'T2-S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first list files for the train and validation sets.\n",
    "train_val_image_file_list = []\n",
    "train_val_label_list = []\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    for filename in glob.iglob( os.path.join(input_train_path,class_names[i]) + '/**/*.png', recursive=True):\n",
    "        # extract patient number and slice\n",
    "        train_val_image_file_list.append(filename)\n",
    "        train_val_label_list.append( i )     \n",
    "\n",
    "print(len(train_val_image_file_list), len(train_val_label_list))\n",
    "print(train_val_image_file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permutation\n",
    "permutation = np.random.permutation( len(train_val_label_list) )\n",
    "train_val_images_files=[train_val_image_file_list[i] for i in permutation]\n",
    "train_val_labels=[train_val_label_list[i] for i in permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the train set in two parts : train and validation\n",
    "VALIDATION_RATIO = 0.2\n",
    "nb_train = int( len(train_val_label_list) * (1 - VALIDATION_RATIO) )\n",
    "# list images for both datasets\n",
    "train_images_files = train_val_images_files[:nb_train]\n",
    "train_labels =train_val_labels[:nb_train]\n",
    "\n",
    "val_images_files = train_val_images_files[nb_train:]\n",
    "val_labels = train_val_labels[nb_train:]\n",
    "\n",
    "# for labels also convert them to numpy\n",
    "y_train = np.asarray(train_labels)\n",
    "y_val = np.asarray(val_labels)\n",
    "\n",
    "print( train_val_images_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading images \n",
    "def ReadImages(images_files):\n",
    "    X = []\n",
    "    for index in tqdm(range(len(images_files))):\n",
    "        image_read = cv2.imread(images_files[index], cv2.IMREAD_COLOR)\n",
    "        image_read = cv2.resize(image_read, dsize = (desired_width, desired_height), interpolation = cv2.INTER_LINEAR)\n",
    "        X.append(image_read)\n",
    "    X = np.asarray(X, dtype=np.uint8)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "X_train_rgb = ReadImages(train_images_files)\n",
    "X_val_rgb   = ReadImages(val_images_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B2 - Check the output data size and type**\n",
    "We recommend to check as often as needed the shape and type of data.\n",
    "\n",
    "Don't go further if you can't see what is inside array and tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information\n",
    "print(\" Shape : \" , X_train_rgb.shape, X_val_rgb.shape)\n",
    "print(\" Type  : %s  %s\"%(X_train_rgb.dtype, X_val_rgb.dtype))\n",
    "print(\" Max   : %d  %d\"%(X_train_rgb.max(), X_val_rgb.max()))\n",
    "print(\" Min   : %d  %d\"%(X_train_rgb.min(), X_val_rgb.min()))\n",
    "\n",
    "print(\" Labels Shape : \" , y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B3 - Displaying images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, aspect=\"equal\", cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_x = 5\n",
    "nb_y = 5\n",
    "plt.figure(figsize=(15,15*nb_x/nb_y), dpi=100)\n",
    "for i in range(1, nb_x * nb_y +1):\n",
    "    plt.subplot(nb_x, nb_y ,i)\n",
    "    plot_image( X_train_rgb[i-1] )\n",
    "    plt.title(class_names[y_train[i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B4 - Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for test set\n",
    "test_image_file_list = []\n",
    "test_label_list = []\n",
    "           \n",
    "for i in range(len(class_names)):\n",
    "    for filename in glob.iglob( os.path.join(input_test_path,class_names[i]) + '/**/*.png', recursive=True):\n",
    "        # extract patient number and slice\n",
    "        test_image_file_list.append(filename)\n",
    "        test_label_list.append( i )            \n",
    "\n",
    "print(len(test_image_file_list), len(test_label_list))\n",
    "\n",
    "X_test_rgb = ReadImages(test_image_file_list)\n",
    "y_test = np.asarray(test_label_list)\n",
    "\n",
    "print(\" Shape : \" , X_test_rgb.shape, y_test.shape)\n",
    "print(\" Type  : %s  %s\"%(X_test_rgb.dtype, y_test.dtype))\n",
    "print(\" Max   : %d  %d\"%(X_test_rgb.max(), y_test.max()))\n",
    "print(\" Min   : %d  %d\"%(X_test_rgb.min(), y_test.min()))\n",
    "\n",
    "nb_x = 5\n",
    "nb_y = 5\n",
    "plt.figure(figsize=(15,15*nb_x/nb_y), dpi=100)\n",
    "for i in range(1, nb_x * nb_y +1):\n",
    "    plt.subplot(nb_x, nb_y ,i)\n",
    "    plot_image( X_test_rgb[i-1] )\n",
    "    plt.title(class_names[y_test[i-1]])\n",
    "# here we have not shuffle filenames :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify shapes that should look like ( batch_size, desired_height, desired_width, 3 ) and (batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **B5 - Convert the data shape and type**\n",
    "The _png_ files are RGB, we convert them to gray images for each set (train, validation and test) in order to be compatible with all tested approaches.\n",
    "\n",
    "We also divide intensity values by 255 to bound range in [0;1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the three data set : rgb to gray scale images, then normalization\n",
    "\n",
    "X_train = X_train_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_train = np.expand_dims(X_train,-1)\n",
    "\n",
    "X_val = X_val_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_val = np.expand_dims(X_val,-1)\n",
    "\n",
    "X_test = X_test_rgb[:,:,:,1].astype(np.float32)/255\n",
    "X_test = np.expand_dims(X_test,-1)\n",
    "\n",
    "print(\" Shape : \", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\" Type  : %s  %s %s\"%(X_train.dtype, X_val.dtype, X_test.dtype))\n",
    "print(\" Max   : %d %d %d\"%(X_train.max(), X_val.max(), X_test.max()) )\n",
    "print(\" Min   : %d %d %d\"%(X_train.min(),X_val.min(), X_test.min()) )\n",
    "plot_image( X_test[0].squeeze() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other way of representing labels is the one hot encoding form. MAny mathematical function can easly be generalized using this notation.\n",
    "If they are 4 different labels, the sparse notation becomes:\n",
    "   - label '0' --> 1 0 0 0\n",
    "   - label '1' --> 0 1 0 0\n",
    "   - label '2' --> 0 0 1 0\n",
    "   - label '3' --> 0 0 0 1\n",
    "\n",
    "This conversion can be done using a dedicated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(\" Original (sparse) shape  : \", y_train.shape, y_val.shape, y_test.shape)\n",
    "print(\" Categorical shape  : \", y_train_cat.shape, y_val_cat.shape, y_test_cat.shape)\n",
    "print(\" Example : \", y_train[0,], \" --> \" ,y_train_cat[0,])\n",
    "\n",
    "# Now changing name :\n",
    "y_train = y_train_cat\n",
    "y_val = y_val_cat\n",
    "y_test = y_test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Question**\n",
    "- What means the 9 for the last axis of y_train_cat, y_val_cat, y_test_cat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**All done, go to the classification step!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function for displaying reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example : ClassificationReport(model, X_test, y_test_cat , class_names)\n",
    "def ClassificationReport(model, X_test, y_test_cat, labels_name):  \n",
    "    # predict \n",
    "    print(\"Prediction on test images\")\n",
    "    predicted = model.predict(X_test)\n",
    "    y_pred = np.argmax(predicted, axis=-1)\n",
    "    y_test = np.argmax(y_test_cat, axis=-1)\n",
    "    \n",
    "    print(\"Classification report :\\n%s\\n\"\n",
    "         % (metrics.classification_report(y_test, y_pred)) )\n",
    "    cm = metrics.confusion_matrix( y_test, y_pred )\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=labels_name)\n",
    "    disp.plot()\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **C - Classification with random forest**\n",
    "    \n",
    "First, we want to use a random forest to perform the classification. A random forest is an ensemble of decision trees. \n",
    "    \n",
    "Each decision tree takes as input a feature vector. In our application, the feature vector is the grey-level values of the 32x32 (=1024 features) patch centered on each slice. \n",
    "    \n",
    "During training, a matrix (X_train_crop) of size Nx1024 (N is the number of training images) is fed to the algorithm to build a decision tree. Learning the decision tree consists in finding the combination of features and cut-off values ($\\alpha_i$) of each node such that the loss is minimal.\n",
    "    \n",
    "During test, the feature vector of one image is given to the learnt decision tree and goes through the first node. If the value of the feature (i.e. $f_5$ in the example) is higher than the cut-off value ($\\alpha_0$), we proceed to the left branch, otherwise we proceed to the righ branch. This process is repeated until a leaf is reached, which will correspond to the predicted class.\n",
    "    \n",
    "A random forest is an ensemble of decision trees. M (M = 10 in the code below) decisions trees are learnt, each one from a subsample of the N training images. The final prediction is performed by a majority vote over the M predictions.\n",
    "            \n",
    "<center><img src=\"./figures/decision_tree.png\" alt=\"Decision Tree\" style=\"width: 50%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we prepare data for scikit-learn\n",
    "\n",
    "# crop around the image center\n",
    "crop_min = 32-16\n",
    "crop_max = 32+16 +1 # '+1' is for python indexes : [0:10] corresponds to indexes 0 to 9 \n",
    "\n",
    "# flattening for scikit-learn (ie a 64x64 image -> a vector of 4096 elements)\n",
    "X_train_crop = []\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    X_train_crop.append( X_train[i,crop_min:crop_max,crop_min:crop_max, 0].flatten() )\n",
    "X_train_crop = np.asarray( X_train_crop )\n",
    "\n",
    "X_val_crop = []\n",
    "for i in range(0, X_val.shape[0]):\n",
    "    X_val_crop.append( X_val[i,crop_min:crop_max,crop_min:crop_max, 0].flatten() )\n",
    "X_val_crop = np.asarray( X_val_crop )\n",
    "\n",
    "X_test_crop = []\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    X_test_crop.append( X_test[i,crop_min:crop_max,crop_min:crop_max, 0].flatten() )\n",
    "X_test_crop = np.asarray( X_test_crop )\n",
    "\n",
    "print(\" X_train_crop shape : \", X_train_crop.shape )\n",
    "print(\" y_train      shape : \", y_train.shape )\n",
    "print(\" X_val_crop   shape : \", X_val_crop.shape )\n",
    "print(\" y_val        shape : \", y_val.shape )\n",
    "print(\" X_test_crop  shape : \", X_test_crop.shape )\n",
    "print(\" y_test       shape : \", y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "classifier = ensemble.RandomForestClassifier(n_estimators=10, max_depth=4, max_features ='auto', n_jobs=4, verbose=1)\n",
    "#classifier = ensemble.GradientBoostingClassifier(n_estimators=10, max_depth=5, max_features ='auto', verbose=1) # can be very long !\n",
    "\n",
    "print(\"Train model\")\n",
    "classifier.fit(X_train_crop, y_train)\n",
    "\n",
    "print(\"Compute predictions\")\n",
    "y_pred = classifier.predict(X_test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print(\"Classification report for classifier Random Forest :\\n%s\\n\"\n",
    "         % (metrics.classification_report(np.argmax(y_test, axis=-1), np.argmax(y_pred, axis=-1) )) )\n",
    "cm = metrics.confusion_matrix( np.argmax(y_test, axis=-1), np.argmax(y_pred, axis=-1) )\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=class_names)\n",
    "disp.plot()\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- Modify the classification parameters to improve the accuracy. Objective : accuracy > 95%\n",
    "- What do you think of the level of performance achieved?\n",
    "- Try to test other classification approaches from [scikit-learn](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> **D - Classification with Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **D1 - Multi-Layer Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enter step by step into the world of deep learning, we will first look at the problem of classification of the [IXI dataset](https://brain-development.org/ixi-dataset/) using one of the simple network dedicated to such task, i.e. the multi-layer perceptron (also called a fully connected network - FCN). The corresponding architecture is provided below.\n",
    "\n",
    "<center><img src=\"./figures/architecture_fcn_en.png\" alt=\"Architecture FCN\" style=\"width: 50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the cell below will allow you to load in memory the functions (actions) and constants that you will use afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of functions that will be used from the standard  tensorflow library\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "\n",
    "# Definition of constants\n",
    "input_shape = (desired_height,desired_width,1)\n",
    "input_shape_rgb = (desired_height,desired_width,3)\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff7800\"><b>Comment</b>: You can play with the value of some typical parameters by executing the cell below. During your session, feel free to go back to this cell, change some values and re-run the code from this cell to see the impact on the results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of variables that you will have to play with during the session to study their influence\n",
    "\n",
    "# Variables related to the FCN architecture\n",
    "nb_mlp_neurons = 16    # Number of neurons for one layer\n",
    "\n",
    "# Variables related to the optimization process\n",
    "batch_size    = 16     # Number of images for each batch during the training process \n",
    "nb_epochs     = 5      # number of epochs used during the training process\n",
    "learning_rate = 0.001  # learning rate used during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a FCN (Fully Connected Network) model using a sequential declaration allowed by the tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code allows you to free up memory space \n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model\n",
    "\n",
    "# Define a FCN model from sequential declaration\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(nb_mlp_neurons, activation='relu'),  # a fully-connected layer with nb_mlp_neurons hidden units\n",
    "  tf.keras.layers.Dense(num_classes, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "# Display the network structure on the console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- Analyze the description of the network that appears above and try to make the link with the diagram of the fully connected network given on the figure above.    \n",
    "- How many parameters does this network have in total ?\n",
    "- Can you explain the 153 parameters that appear for the last layer (named dense_1) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start your first training by running the following cell !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile (define the optimization scheme) the model to be ready for training\n",
    "model.compile(\n",
    "  optimizer=optimizers.SGD(learning_rate=learning_rate),\n",
    "  #optimizer=optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999),\n",
    "  \n",
    "  loss=losses.CategoricalCrossentropy(),\n",
    "  #loss=losses.CategoricalHinge(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "  X_train, y_train_cat,\n",
    "  validation_data=(X_val, y_val_cat),\n",
    "  batch_size=batch_size,  \n",
    "  epochs=nb_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff7800\"><b>Comment</b>: During a training session, two sub-databases are used. The images belonging to the training sub-database (train) are directly used to update the network weights being optimised. The images belonging to the validation sub-database (valid) are used to evaluate the performance of the network on \"neutral\" (unused) images during optimisation. In the present configuration, the weights obtained at the end of the training process will be taken as the optimal ones.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, you will apply the network to a sub-database called the test database. This database was never seen before by the network and will allow you to evaluate the performance of your network in an unbiased way. Ideally, we would like these results to be close to those obtained on the validation database.\n",
    "\n",
    "Run the cell below to assess the performance of the previously learned network on the test database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall accuracy of the model on the test dataset \n",
    "test_loss, test_accuray = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', test_accuray)\n",
    "\n",
    "# Compute and display standart classication metrics, i.e. precision, recall, f1-score, support and confusion matrix \n",
    "ClassificationReport(model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- Now that you have completed your first learning from scratch, go back to the cell that allows you to modify the network parameters in order to perform new learning to see the influence of certain parameters. For example, you can play on the following parameters: <em>nb_epochs</em>, <em>learning_rate</em> et <em>nb_mlp_neurons</em>. To explore efficiently those parameters, multiply or divide by 2 or 10 their values.\n",
    "- Add an additional fully connected layers by adding \"tf.keras.layers.Dense()\" functions in the sequential declaration of the model.\n",
    "- Try to get the best possible performance (ie accuracy > 97% ).\n",
    "- Compare the scores between the FCN and the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **D2 - Convolutional Neural Network (CNN)**\n",
    "\n",
    "In order to try to improve the results, we are now looking at convolutional neural networks, of which an example architecture is provided below.\n",
    "\n",
    "<center><img src=\"./figures/architecture_cnn_en.png\" alt=\"Architecture CNN\" style=\"width: 90%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff7800\"><b>Comment</b>: The interest of these kinds of networks is to add a succession of convolution layers in order to create an efficient (discriminating) representation of the data before the classification stage. These convolution layers also allow to significantly reduce the number of network parameters, which is a determining factor in the perspective of deploying these solutions in clinical routine (the fewer the parameters, the faster the network will run).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff7800\"><b>Comment</b>: You can play with the value of some typical parameters by executing the cell below. During your session, feel free to go back to this cell, change some values and re-run the code from this cell to see the impact on the results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of variables that you will have to play with during the session to study their influence\n",
    "\n",
    "# Variables related to the FCN architecture\n",
    "nb_mlp_neurons = 16    # Number of neurons for one layer\n",
    "nb_cnn_neurons = 16    # number of feature maps\n",
    "\n",
    "# Variables related to the optimization process\n",
    "batch_size    = 16     # Number of images for each batch during the training process \n",
    "nb_epochs     = 5      # number of epochs used during the training process\n",
    "learning_rate = 0.001  # learning rate used during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a CNN (Convolutional Neural Network) model using a sequential declaration allowed by the tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code allows you to free up memory space \n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model\n",
    "    \n",
    "# Define a CNN model from sequential declaration    \n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons//2, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(nb_cnn_neurons*2, 3, activation='relu', padding='same'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(nb_mlp_neurons, activation='relu'),  #128\n",
    "  tf.keras.layers.Dense(num_classes, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "# Display the network structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- Analyze the description of the network that appears after the execution of the cell above and try to make the link with the CNN diagram given above.    \n",
    "- How many parameters does this network have in total ?\n",
    "- Compare this value with the one of the FCN studied above.\n",
    "- How can you explain this favourable difference ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile (define the optimization scheme) the model to be ready for training\n",
    "model.compile(\n",
    "  optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "  loss=tf.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  validation_data=(X_val, y_val),\n",
    "  batch_size=batch_size,  \n",
    "  epochs=nb_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to evaluate the performance of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall accuracy of the model on the test dataset \n",
    "test_loss, test_accuray = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', test_accuray)\n",
    "\n",
    "# Compute and display standart classication metrics, i.e. precision, recall, f1-score, support and confusion matrix \n",
    "ClassificationReport(model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- From the cells allowing to modify the network architecture as well as the optimisation parameters, try to get the best possible performance (if necessary...).\n",
    "- Compare the scores between the CNN and FCN models.\n",
    "- Is there a big difference? What conclusions can you draw from this ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> **D3 - Residual deep network and transfer learning (fine tuning)**\n",
    "    \n",
    "We will now conclude this hands-on session by working on one of the most powerful networks currently used for image classification, namely the resnet model whose architecture is recalled below. \n",
    "\n",
    "<center><img src=\"./figures/architecture_resnet_en.png\" alt=\"Architecture ResNet\" style=\"width: 60%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture is very deep and potentially challenging to train. In order to be efficient, the use of such an architecture for a given application is usually done through a particular strategy, called transfer learning.\n",
    "\n",
    "The idea behind transfer learning is to reuse the weights of a part of the network that have already been learned from large databases, such as imagenet. This section of the architecture, named base, corresponds to the set of layers before the classification part. Thus, for a given application, the base part of the resnet with the associated pre-trained weights is selected first. A dedicated classification subnetwork (i.e. a fully connected network) is then added at the end to form a new architecture. During the training phase, only the weights of the network's head (i.e. the classification part) are optimized, making the learning process faster.\n",
    "\n",
    "In the remainder of this hands on session, you will create such a resnet-based architecture using transfer learning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff7800\"><b>Comment</b>: You can play with the value of some typical parameters by executing the cell below. During your session, feel free to go back to this cell, change some values and re-run the code from this cell to see the impact on the results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of variables that you will have to play with during the session to study their influence\n",
    "\n",
    "# Variables related to the FCN architecture\n",
    "nb_mlp_neurons = 16    # Number of neurons for one layer\n",
    "\n",
    "# Variables related to the optimization process\n",
    "batch_size  = 16       # Number of images for each batch during the training process \n",
    "nb_epochs   = 5          # number of epochs used during the training process\n",
    "learning_rate = 0.001  # learning rate used during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a resnet-based model using a predifined structure from the tensorflow library.\n",
    "This code is explained in the figure below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code allows you to free up memory space \n",
    "if 'model' in locals(): \n",
    "    print(\"deleting model\")\n",
    "    del model\n",
    "\n",
    "# transfert learning / fine tuning head (classification part) only\n",
    "print(\" ResNET50 network is trained with RGB natural images, so we use the RGB train and test datasets.\")\n",
    "    \n",
    "# Define input and preprocessed them to fit the conditions used for the predifined resent model \n",
    "inputs = tf.keras.layers.Input(shape=input_shape_rgb)\n",
    "x = tf.cast(inputs, tf.float32)\n",
    "outputs_preproc = tf.keras.applications.resnet_v2.preprocess_input(x)  # pixels values are changed to [-1;1]\n",
    "\n",
    "# Define the ResNet architecture and dowload weights trained on imagenet\n",
    "baseModel = tf.keras.applications.ResNet50V2(\n",
    "    weights=\"imagenet\", # use \"imagenet\" to use imagenet resnet weights, or None (!! no \" \") for random initialization of network parameters\n",
    "    include_top=False,  # don't include the fully-connected layers (classification part) at the top of ResNet\n",
    "    input_tensor=tf.keras.layers.Input(shape=input_shape_rgb))\n",
    "\n",
    "# Link outputs of preprocessing as intput of baseModel\n",
    "outputs_base = baseModel(outputs_preproc)\n",
    "\n",
    "# Create the model of preproc + resnet without 'top' or 'head'\n",
    "baseModel_preproc = tf.keras.Model(inputs=[inputs], outputs=[outputs_base]) \n",
    "    \n",
    "# Define the head model i.e. the part that is flattenning alls features extracted before and then classify the data\n",
    "# and which is fully connected (MLP...) in order to obtain the classification\n",
    "headModel = baseModel_preproc.output\n",
    "headModel = tf.keras.layers.Flatten(name = \"flatten\")(headModel)\n",
    "headModel = tf.keras.layers.Dense(nb_mlp_neurons, activation = \"relu\")(headModel)\n",
    "outputs_head = tf.keras.layers.Dense(num_classes, activation = \"softmax\")(headModel)\n",
    "\n",
    "# Create the final resnet-base model\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=outputs_head)\n",
    "\n",
    "# allow baseModel weights to be trainable ? (if True, the training step will be long i.e. 1 to 3 minutes per epoch)\n",
    "baseModel.trainable = False\n",
    "\n",
    "# Display the network structure on the console\n",
    "model.summary()\n",
    "\n",
    "# uncomment to see ResNet50 architecture (it is a deep network :p )\n",
    "#baseModel.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./figures/ResNet_TransferLearning.png\" alt=\"ResNet Transfer Learning schme\" style=\"width: 60%\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- Analyze the description of the network that appears after the execution of the cell above and try to make the link with the diagram given above.    \n",
    "- How many parameters does this network have in total ?\n",
    "- Compare this value with the one of the CNN studied above.\n",
    "- Where do the differences come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile (define the optimization scheme) the model to be ready for training\n",
    "model.compile(\n",
    "  optimizer=optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999),\n",
    "  loss=tf.losses.CategoricalCrossentropy(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "  X_train_rgb, y_train,\n",
    "  validation_data=(X_val_rgb, y_val),\n",
    "  batch_size=batch_size,  \n",
    "  epochs=nb_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to evaluate the performance of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall accuracy of the model on the test dataset \n",
    "test_loss, test_accuray = model.evaluate(X_test_rgb, y_test, verbose=1)\n",
    "print('Test accuracy:', test_accuray)\n",
    "\n",
    "# Compute and display standart classication metrics, i.e. precision, recall, f1-score, support and confusion matrix \n",
    "ClassificationReport(model, X_test_rgb, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions**\n",
    "- From the cells allowing to modify the network architecture as well as the optimisation parameters, try to get the best possible performance.\n",
    "- The training time is longer, why ? \n",
    "- Is the accuracy better than previous and smaller network? Have a look at the number of trainable parameters...\n",
    "- [long test] Is the accuracy better if all weights of the network can be trained ? (i.e. including the ones of baseModel)\n",
    "- [very long test] How to assess the role of the initial weights coming from imagenet ? (by the way : what is imagenet?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the best weights according val_loss\n",
    "# don't execute this cell! \n",
    "#   copy and adapt it in the previous cells...\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback_earlystopping = EarlyStopping(\n",
    "    monitor='val_loss', #'val_loss'\n",
    "    mode ='auto', # use 'min' or 'auto' if val_loss\n",
    "    patience=NBPATIENCE_EPOCHS,\n",
    "    restore_best_weights=True  # at the end of fitting, restore best model weights \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  X_train_rgb, y_train,  # or X_train, y_train,\n",
    "  validation_data=(X_val_rgb, y_val), # or (X_val, y_val)\n",
    "  batch_size=batch_size,  \n",
    "  epochs=nb_epochs,\n",
    "  callbacks=[callback_earlystopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
