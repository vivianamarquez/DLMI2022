{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hands-on Deep Learning â€“ Gradient and AutoGrad with TensorFlow/Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This hands-on was created by Thomas Grenier (TensorFlow) and Fabien Millioz (PyTorch), CREATIS, [deepimaging2019](https://deepimaging2019.sciencesconf.org/)_\n",
    "\n",
    "thomas.grenier@creatis.insa-lyon.fr\n",
    "\n",
    "fabien.millioz@creatis.insa-lyon.fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> A - Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**if no error occurs, your working environment is ok and you can go to next part,\n",
    "> else ... call an assistant for help!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> B - Tensors of TensorFlow   \n",
    "\n",
    "This section presents the tensors of TensorFlow and the link with numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B1- Tensorflow\n",
    "\n",
    "All tensorflow 2 variables are declared and interpreted.\n",
    "As example, a 2 by 2 random unfirom matrix with values in the range [-1;1] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tf = tf.random.uniform(shape=[2, 2], minval=-1.0, maxval=1.0, seed=42)\n",
    "print(a_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.multiply(a_tf, 10)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_tf * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B2- Link with numpy\n",
    "\n",
    "We can use numpy array to do the same, and mix tensorflow and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_np = np.random.rand(2, 2)\n",
    "print(a_np)\n",
    "\n",
    "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
    "tensor = tf.multiply(a_np, 10)\n",
    "print(tensor)\n",
    "\n",
    "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
    "print(np.add(tensor, 1))\n",
    "\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> C- Calculate a simple gradient\n",
    "We can calculate gradients with automatic differentiation of the function $ c(a) = \\frac{1}{n.m} \\sum_{i,j}^{n,m} 3(a_{ij}+2)^2 $. \n",
    "\n",
    "Let us note $ b(a) = 3(a_{ij}+2)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=[2, 2])\n",
    "print(\"a=\", a.numpy())\n",
    "b = 3 * (a + 2) ** 2\n",
    "print(\"b=\", b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define $ c(a) $ the mean of $ b(a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.reduce_mean(b, name=\"c\")\n",
    "print(\"Tensorflow mean : \", c)\n",
    "print(\"Numpy mean      : \", np.mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We remind that the gradient of $c(a)$ is** $ \\nabla c(a) = \\frac{3(a+2)}{2} $.\n",
    "\n",
    "Here we evaluate it automatically from $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform(shape=[2, 2])\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    b = 3 * (x + 2) ** 2\n",
    "    y = tf.reduce_mean(b)\n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "print(\" TF auto differenciation gradient \\n \", dy_dx)\n",
    "print(\" \\n Manually expressed gradient   \\n \", 3 * (x + 2) / 2)\n",
    "\n",
    "print(\" \\n \\n by the way db_dx = \", g.gradient(b, x))\n",
    "\n",
    "del g  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> D- Gradient descent example\n",
    "\n",
    "Here we plan to minimize the cost function $L(y, \\hat{y}) = (y - \\hat{y})^2$ according to $w$ and $b$ the weights and biais of a neuron.\n",
    "\n",
    "The gradients $\\frac{\\partial L(y - h(x))}{\\partial w}$ and $\\frac{\\partial L(y - h(x))}{\\partial b}$ are (automatically) computed with:\n",
    "- $h(x) = \\sigma(w.x + b)$\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $L(y, \\hat{y}) = (y - \\hat{y})^2$ (quadratic error)\n",
    "- $y = 0.2$\n",
    "- $x = 1.5$\n",
    "- $b = -2$\n",
    "- $w = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h) ** 2  # error = (y - h)**2\n",
    "\n",
    "gradients = g.gradient(error, [b, w])  # error.backward()\n",
    "\n",
    "print(\"h      = \", h.numpy())\n",
    "print(\"grad b = \", gradients[0].numpy())\n",
    "print(\"grad w = \", gradients[1].numpy())\n",
    "del g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize $L(y, h(x))$ iteratively.\n",
    "\n",
    "Weights and bias are updated according to their gradients:\n",
    "\n",
    " - $ w = w - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial w}$ \n",
    "\n",
    " - $ b = b - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial b}$ \n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = tf.constant(1.0)  # alpha = 1\n",
    "nb_epochs = 20  # number of epochs\n",
    "# corresponding pyTorch code (Fabien Milloz)\n",
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    with tf.GradientTape(persistent=False) as g:\n",
    "        h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "        error = (y - h) ** 2  # error = (y - h)**2\n",
    "    gradients = g.gradient(error, [b, w])\n",
    "    b.assign(b - alpha * gradients[0])\n",
    "    w.assign(w - alpha * gradients[1])\n",
    "    print(\n",
    "        \"Epoch {} error={:.05f} h={:.05f} w={:.05f} b={:.05f} dE_db={:.05f} dE_dw={:.05f} \".format(\n",
    "            i + 1,\n",
    "            error.numpy(),\n",
    "            h.numpy(),\n",
    "            w.numpy(),\n",
    "            b.numpy(),\n",
    "            gradients[0].numpy(),\n",
    "            gradients[1].numpy(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Question:** Observe the influence of $\\alpha$ for values of 0.01, 0.1, 10 and 100, and try to adapt the number of epochs accordingly.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> E- Hinge Loss example\n",
    "\n",
    "Here we study another loss function for classification : the multiclass Hinge loss. This is the 'SVM' loss.\n",
    "In tensorflow/keras, this loss is available thank to the function tf.keras.losses.CategoricalHinge().\n",
    "\n",
    "  > **Question:** Using Tensorflow online help, give the expression of this loss.\n",
    "    \n",
    "  > **Question:** Give the tensorflow code used to implement this function (explore the source code of tensorflow and more specifically the categorcial_hinge function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1.0, 0.0 , 0.0]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[.0, 1.0, 0.0]], dtype=tf.float32) #before SoftMax  : bSM\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "print(\"y_pred\", y_pred.numpy())\n",
    "h = tf.keras.losses.CategoricalHinge()\n",
    "#h=tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h =\", h( y_true, y_pred ).numpy() )\n",
    "print(\"y_true shape\", y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > **Question:** What do the values of y_pred and y_true mean ?\n",
    "  \n",
    "  > **Question:** With equations of the Hinge loss, verify the previous results.\n",
    "  \n",
    "The loss value is interesting but its derivatives are mandatory to be used within a gradient descent scheme. The following code calculates automatically these derivatives according each y_pred.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred, y_pred_bSM))\n",
    "  y_pred=tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient( h_graph, (y_pred,y_pred_bSM) )\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other realistic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype=tf.float32)\n",
    "#y_pred_bSM = tf.constant([[0.0575, 0.0195, 0.4029, 0.0945, 0.0288, 0.0031, 0.0908, 0.0372, 0.2652]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[1.26908707619, 0.61077165603, -0.7032195329, 0.92837673425, -0.5457400083, -0.6242400407, 0.45880278945, 0.64756596088, -0.0037845533]], dtype=tf.float32)\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "h_hinge = tf.keras.losses.CategoricalHinge()\n",
    "h_cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h_hinge =\", h_hinge(y_true,y_pred).numpy() )\n",
    "print(\"h_cce =\", h_cce(y_true, y_pred).numpy() )\n",
    "print(\"y_pred \", y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred,y_pred_bSM))\n",
    "  y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h_hinge(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, (y_pred,y_pred_bSM))\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(y_pred)\n",
    "  h_graph = h_cce(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, y_pred)\n",
    "print(dh_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
