{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hands-on Deep Learning â€“ Gradient and AutoGrad with TensorFlow/Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This hands-on was created by Thomas Grenier (TensorFlow) and Fabien Millioz (PyTorch), CREATIS, [deepimaging2019](https://deepimaging2019.sciencesconf.org/)_\n",
    "\n",
    "thomas.grenier@creatis.insa-lyon.fr\n",
    "\n",
    "fabien.millioz@creatis.insa-lyon.fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> A - Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T20:57:11.174675Z",
     "iopub.status.busy": "2022-07-05T20:57:11.174254Z",
     "iopub.status.idle": "2022-07-05T20:57:12.592568Z",
     "shell.execute_reply": "2022-07-05T20:57:12.591782Z",
     "shell.execute_reply.started": "2022-07-05T20:57:11.174602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 20:57:11.331905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**if no error occurs, your working environment is ok and you can go to next part,\n",
    "> else ... call an assistant for help!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> B - Tensors of TensorFlow   \n",
    "\n",
    "This section presents the tensors of TensorFlow and the link with numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B1- Tensorflow\n",
    "\n",
    "All tensorflow 2 variables are declared and interpreted.\n",
    "As example, a 2 by 2 random unfirom matrix with values in the range [-1;1] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T20:57:16.723653Z",
     "iopub.status.busy": "2022-07-05T20:57:16.723134Z",
     "iopub.status.idle": "2022-07-05T20:57:17.383562Z",
     "shell.execute_reply": "2022-07-05T20:57:17.382576Z",
     "shell.execute_reply.started": "2022-07-05T20:57:16.723623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9045429  0.35481548]\n",
      " [0.5906365  0.51156354]], shape=(2, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 20:57:16.725133: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-05 20:57:16.726125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-05 20:57:16.748581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.748938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-07-05 20:57:16.748984: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-05 20:57:16.751543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-05 20:57:16.751643: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-05 20:57:16.753688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-05 20:57:16.754099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-05 20:57:16.756737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-05 20:57:16.758191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-05 20:57:16.762129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-05 20:57:16.762264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.762549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.762748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-05 20:57:16.763734: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-05 20:57:16.763992: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-05 20:57:16.764140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.764423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-07-05 20:57:16.764450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-05 20:57:16.764472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-05 20:57:16.764485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-05 20:57:16.764498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-05 20:57:16.764510: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-05 20:57:16.764523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-05 20:57:16.764536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-05 20:57:16.764549: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-05 20:57:16.764623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.764904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:16.765174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-05 20:57:16.765224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-05 20:57:17.375233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-05 20:57:17.375272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-05 20:57:17.375280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-05 20:57:17.375499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:17.375805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:17.376060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 20:57:17.376274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 35 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "a_tf = tf.random.uniform(shape=[2, 2], minval=-1.0, maxval=1.0, seed=42)\n",
    "print(a_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T20:57:17.384971Z",
     "iopub.status.busy": "2022-07-05T20:57:17.384730Z",
     "iopub.status.idle": "2022-07-05T20:57:17.389432Z",
     "shell.execute_reply": "2022-07-05T20:57:17.388789Z",
     "shell.execute_reply.started": "2022-07-05T20:57:17.384950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.045429  3.5481548]\n",
      " [5.906365  5.1156354]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.multiply(a_tf, 10)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:21.504713Z",
     "iopub.status.busy": "2022-07-05T21:01:21.504313Z",
     "iopub.status.idle": "2022-07-05T21:01:21.509143Z",
     "shell.execute_reply": "2022-07-05T21:01:21.508522Z",
     "shell.execute_reply.started": "2022-07-05T21:01:21.504684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.045429  3.5481548]\n",
      " [5.906365  5.1156354]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a_tf * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B2- Link with numpy\n",
    "\n",
    "We can use numpy array to do the same, and mix tensorflow and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:22.411324Z",
     "iopub.status.busy": "2022-07-05T21:01:22.410935Z",
     "iopub.status.idle": "2022-07-05T21:01:22.418923Z",
     "shell.execute_reply": "2022-07-05T21:01:22.418302Z",
     "shell.execute_reply.started": "2022-07-05T21:01:22.411297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94490572 0.7810241 ]\n",
      " [0.09161492 0.4269503 ]]\n",
      "TensorFlow operations convert numpy arrays to Tensors automatically\n",
      "tf.Tensor(\n",
      "[[9.44905717 7.810241  ]\n",
      " [0.91614919 4.26950304]], shape=(2, 2), dtype=float64)\n",
      "And NumPy operations convert Tensors to numpy arrays automatically\n",
      "[[10.44905717  8.810241  ]\n",
      " [ 1.91614919  5.26950304]]\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[9.44905717 7.810241  ]\n",
      " [0.91614919 4.26950304]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_np = np.random.rand(2, 2)\n",
    "print(a_np)\n",
    "\n",
    "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
    "tensor = tf.multiply(a_np, 10)\n",
    "print(tensor)\n",
    "\n",
    "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
    "print(np.add(tensor, 1))\n",
    "\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> C- Calculate a simple gradient\n",
    "We can calculate gradients with automatic differentiation of the function $ c(a) = \\frac{1}{n.m} \\sum_{i,j}^{n,m} 3(a_{ij}+2)^2 $. \n",
    "\n",
    "Let us note $ b(a) = 3(a_{ij}+2)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:25.032860Z",
     "iopub.status.busy": "2022-07-05T21:01:25.032459Z",
     "iopub.status.idle": "2022-07-05T21:01:25.041096Z",
     "shell.execute_reply": "2022-07-05T21:01:25.040452Z",
     "shell.execute_reply.started": "2022-07-05T21:01:25.032833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.83705306 0.6542785 ]\n",
      " [0.80893373 0.13885796]]\n",
      "b= [[24.14661  21.135582]\n",
      " [23.670326 13.724138]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=[2, 2])\n",
    "print(\"a=\", a.numpy())\n",
    "b = 3 * (a + 2) ** 2\n",
    "print(\"b=\", b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define $ c(a) $ the mean of $ b(a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:25.545862Z",
     "iopub.status.busy": "2022-07-05T21:01:25.545468Z",
     "iopub.status.idle": "2022-07-05T21:01:25.551733Z",
     "shell.execute_reply": "2022-07-05T21:01:25.550982Z",
     "shell.execute_reply.started": "2022-07-05T21:01:25.545830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow mean :  tf.Tensor(20.669163, shape=(), dtype=float32)\n",
      "Numpy mean      :  20.669163\n"
     ]
    }
   ],
   "source": [
    "c = tf.reduce_mean(b, name=\"c\")\n",
    "print(\"Tensorflow mean : \", c)\n",
    "print(\"Numpy mean      : \", np.mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We remind that the gradient of $c(a)$ is** $ \\nabla c(a) = \\frac{3(a+2)}{2} $.\n",
    "\n",
    "Here we evaluate it automatically from $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:27.901767Z",
     "iopub.status.busy": "2022-07-05T21:01:27.901382Z",
     "iopub.status.idle": "2022-07-05T21:01:27.913757Z",
     "shell.execute_reply": "2022-07-05T21:01:27.913128Z",
     "shell.execute_reply.started": "2022-07-05T21:01:27.901740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TF auto differenciation gradient \n",
      "  tf.Tensor(\n",
      "[[4.1359673 3.4799776]\n",
      " [3.4273653 3.6203592]], shape=(2, 2), dtype=float32)\n",
      " \n",
      " Manually expressed gradient   \n",
      "  tf.Tensor(\n",
      "[[4.1359673 3.4799776]\n",
      " [3.4273653 3.6203594]], shape=(2, 2), dtype=float32)\n",
      " \n",
      " \n",
      " by the way db_dx =  tf.Tensor(\n",
      "[[16.543869 13.91991 ]\n",
      " [13.709461 14.481437]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform(shape=[2, 2])\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    b = 3 * (x + 2) ** 2\n",
    "    y = tf.reduce_mean(b)\n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "print(\" TF auto differenciation gradient \\n \", dy_dx)\n",
    "print(\" \\n Manually expressed gradient   \\n \", 3 * (x + 2) / 2)\n",
    "\n",
    "print(\" \\n \\n by the way db_dx = \", g.gradient(b, x))\n",
    "\n",
    "del g  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> D- Gradient descent example\n",
    "\n",
    "Here we plan to minimize the cost function $L(y, \\hat{y}) = (y - \\hat{y})^2$ according to $w$ and $b$ the weights and biais of a neuron.\n",
    "\n",
    "The gradients $\\frac{\\partial L(y - h(x))}{\\partial w}$ and $\\frac{\\partial L(y - h(x))}{\\partial b}$ are (automatically) computed with:\n",
    "- $h(x) = \\sigma(w.x + b)$\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $L(y, \\hat{y}) = (y - \\hat{y})^2$ (quadratic error)\n",
    "- $y = 0.2$\n",
    "- $x = 1.5$\n",
    "- $b = -2$\n",
    "- $w = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:30.844714Z",
     "iopub.status.busy": "2022-07-05T21:01:30.844324Z",
     "iopub.status.idle": "2022-07-05T21:01:30.855653Z",
     "shell.execute_reply": "2022-07-05T21:01:30.855045Z",
     "shell.execute_reply.started": "2022-07-05T21:01:30.844689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h      =  0.9241418\n",
      "grad b =  0.10153006\n",
      "grad w =  0.15229508\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h) ** 2  # error = (y - h)**2\n",
    "\n",
    "gradients = g.gradient(error, [b, w])  # error.backward()\n",
    "\n",
    "print(\"h      = \", h.numpy())\n",
    "print(\"grad b = \", gradients[0].numpy())\n",
    "print(\"grad w = \", gradients[1].numpy())\n",
    "del g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize $L(y, h(x))$ iteratively.\n",
    "\n",
    "Weights and bias are updated according to their gradients:\n",
    "\n",
    " - $ w = w - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial w}$ \n",
    "\n",
    " - $ b = b - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial b}$ \n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:33.168868Z",
     "iopub.status.busy": "2022-07-05T21:01:33.168244Z",
     "iopub.status.idle": "2022-07-05T21:01:33.221192Z",
     "shell.execute_reply": "2022-07-05T21:01:33.220035Z",
     "shell.execute_reply.started": "2022-07-05T21:01:33.168840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 error=0.52438 h=0.92414 w=2.84770 b=-2.10153 dE_db=0.10153 dE_dw=0.15230 \n",
      "Epoch 2 error=0.48654 h=0.89753 w=2.65524 b=-2.22984 dE_db=0.12831 dE_dw=0.19246 \n",
      "Epoch 3 error=0.42554 h=0.85233 w=2.40893 b=-2.39404 dE_db=0.16421 dE_dw=0.24631 \n",
      "Epoch 4 error=0.32713 h=0.77195 w=2.10687 b=-2.59542 dE_db=0.20138 dE_dw=0.30206 \n",
      "Epoch 5 error=0.19148 h=0.63758 w=1.80353 b=-2.79765 dE_db=0.20223 dE_dw=0.30334 \n",
      "Epoch 6 error=0.07669 h=0.47693 w=1.59628 b=-2.93582 dE_db=0.13817 dE_dw=0.20726 \n",
      "Epoch 7 error=0.02818 h=0.36786 w=1.47917 b=-3.01388 dE_db=0.07807 dE_dw=0.11710 \n",
      "Epoch 8 error=0.01234 h=0.31107 w=1.40777 b=-3.06149 dE_db=0.04761 dE_dw=0.07141 \n",
      "Epoch 9 error=0.00623 h=0.27892 w=1.36015 b=-3.09323 dE_db=0.03174 dE_dw=0.04762 \n",
      "Epoch 10 error=0.00344 h=0.25865 w=1.32641 b=-3.11572 dE_db=0.02249 dE_dw=0.03374 \n",
      "Epoch 11 error=0.00201 h=0.24488 w=1.30152 b=-3.13232 dE_db=0.01660 dE_dw=0.02490 \n",
      "Epoch 12 error=0.00123 h=0.23504 w=1.28261 b=-3.14492 dE_db=0.01260 dE_dw=0.01890 \n",
      "Epoch 13 error=0.00077 h=0.22776 w=1.26797 b=-3.15469 dE_db=0.00977 dE_dw=0.01465 \n",
      "Epoch 14 error=0.00049 h=0.22223 w=1.25644 b=-3.16237 dE_db=0.00768 dE_dw=0.01152 \n",
      "Epoch 15 error=0.00032 h=0.21794 w=1.24727 b=-3.16849 dE_db=0.00612 dE_dw=0.00917 \n",
      "Epoch 16 error=0.00021 h=0.21457 w=1.23990 b=-3.17340 dE_db=0.00491 dE_dw=0.00737 \n",
      "Epoch 17 error=0.00014 h=0.21189 w=1.23394 b=-3.17737 dE_db=0.00397 dE_dw=0.00596 \n",
      "Epoch 18 error=0.00009 h=0.20975 w=1.22910 b=-3.18060 dE_db=0.00323 dE_dw=0.00485 \n",
      "Epoch 19 error=0.00006 h=0.20801 w=1.22514 b=-3.18324 dE_db=0.00264 dE_dw=0.00396 \n",
      "Epoch 20 error=0.00004 h=0.20660 w=1.22189 b=-3.18541 dE_db=0.00216 dE_dw=0.00325 \n"
     ]
    }
   ],
   "source": [
    "alpha = tf.constant(1.0)  # alpha = 1\n",
    "nb_epochs = 20  # number of epochs\n",
    "# corresponding pyTorch code (Fabien Milloz)\n",
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    with tf.GradientTape(persistent=False) as g:\n",
    "        h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "        error = (y - h) ** 2  # error = (y - h)**2\n",
    "    gradients = g.gradient(error, [b, w])\n",
    "    b.assign(b - alpha * gradients[0])\n",
    "    w.assign(w - alpha * gradients[1])\n",
    "    print(\n",
    "        \"Epoch {} error={:.05f} h={:.05f} w={:.05f} b={:.05f} dE_db={:.05f} dE_dw={:.05f} \".format(\n",
    "            i + 1,\n",
    "            error.numpy(),\n",
    "            h.numpy(),\n",
    "            w.numpy(),\n",
    "            b.numpy(),\n",
    "            gradients[0].numpy(),\n",
    "            gradients[1].numpy(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Question:** Observe the influence of $\\alpha$ for values of 0.01, 0.1, 10 and 100, and try to adapt the number of epochs accordingly.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> E- Hinge Loss example\n",
    "\n",
    "Here we study another loss function for classification : the multiclass Hinge loss. This is the 'SVM' loss.\n",
    "In tensorflow/keras, this loss is available thank to the function tf.keras.losses.CategoricalHinge().\n",
    "\n",
    "  > **Question:** Using Tensorflow online help, give the expression of this loss.\n",
    "    \n",
    "  > **Question:** Give the tensorflow code used to implement this function (explore the source code of tensorflow and more specifically the categorcial_hinge function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:38.205948Z",
     "iopub.status.busy": "2022-07-05T21:01:38.205547Z",
     "iopub.status.idle": "2022-07-05T21:01:38.214723Z",
     "shell.execute_reply": "2022-07-05T21:01:38.214072Z",
     "shell.execute_reply.started": "2022-07-05T21:01:38.205923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred [[0.21194157 0.5761169  0.21194157]]\n",
      "h = 1.3641753\n",
      "y_true shape (1, 3)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1.0, 0.0 , 0.0]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[.0, 1.0, 0.0]], dtype=tf.float32) #before SoftMax  : bSM\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "print(\"y_pred\", y_pred.numpy())\n",
    "h = tf.keras.losses.CategoricalHinge()\n",
    "#h=tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h =\", h( y_true, y_pred ).numpy() )\n",
    "print(\"y_true shape\", y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > **Question:** What do the values of y_pred and y_true mean ?\n",
    "  \n",
    "  > **Question:** With equations of the Hinge loss, verify the previous results.\n",
    "  \n",
    "The loss value is interesting but its derivatives are mandatory to be used within a gradient descent scheme. The following code calculates automatically these derivatives according each y_pred.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:38.539688Z",
     "iopub.status.busy": "2022-07-05T21:01:38.539304Z",
     "iopub.status.idle": "2022-07-05T21:01:38.549329Z",
     "shell.execute_reply": "2022-07-05T21:01:38.548665Z",
     "shell.execute_reply.started": "2022-07-05T21:01:38.539661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.  1.  0.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[-0.28912547  0.36630934 -0.07718389]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred, y_pred_bSM))\n",
    "  y_pred=tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient( h_graph, (y_pred,y_pred_bSM) )\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other realistic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:41.240320Z",
     "iopub.status.busy": "2022-07-05T21:01:41.239934Z",
     "iopub.status.idle": "2022-07-05T21:01:41.250687Z",
     "shell.execute_reply": "2022-07-05T21:01:41.250061Z",
     "shell.execute_reply.started": "2022-07-05T21:01:41.240292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_hinge = 0.926781\n",
      "h_cce = 1.3720545\n",
      "y_pred  [[0.25358546 0.13128695 0.03528275 0.18036644 0.04130046 0.03818236\n",
      "  0.11277747 0.13620754 0.07101061]]\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype=tf.float32)\n",
    "#y_pred_bSM = tf.constant([[0.0575, 0.0195, 0.4029, 0.0945, 0.0288, 0.0031, 0.0908, 0.0372, 0.2652]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[1.26908707619, 0.61077165603, -0.7032195329, 0.92837673425, -0.5457400083, -0.6242400407, 0.45880278945, 0.64756596088, -0.0037845533]], dtype=tf.float32)\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "h_hinge = tf.keras.losses.CategoricalHinge()\n",
    "h_cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h_hinge =\", h_hinge(y_true,y_pred).numpy() )\n",
    "print(\"h_cce =\", h_cce(y_true, y_pred).numpy() )\n",
    "print(\"y_pred \", y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:41.431203Z",
     "iopub.status.busy": "2022-07-05T21:01:41.430846Z",
     "iopub.status.idle": "2022-07-05T21:01:41.440083Z",
     "shell.execute_reply": "2022-07-05T21:01:41.439457Z",
     "shell.execute_reply.started": "2022-07-05T21:01:41.431180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.  0.  0.  1.  0.  0.  0.  0.  0.]], shape=(1, 9), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.23501818  0.0096127   0.00258337  0.1935727   0.00302398  0.00279568\n",
      "   0.00825746  0.00997298  0.00519933]], shape=(1, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred,y_pred_bSM))\n",
    "  y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h_hinge(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, (y_pred,y_pred_bSM))\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T21:01:41.597505Z",
     "iopub.status.busy": "2022-07-05T21:01:41.597137Z",
     "iopub.status.idle": "2022-07-05T21:01:41.606429Z",
     "shell.execute_reply": "2022-07-05T21:01:41.605726Z",
     "shell.execute_reply.started": "2022-07-05T21:01:41.597480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.9434438  1.         1.         1.         1.         1.\n",
      "   1.         1.         1.       ]], shape=(1, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(y_pred)\n",
    "  h_graph = h_cce(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, y_pred)\n",
    "print(dh_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
